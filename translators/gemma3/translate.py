#!/usr/bin/env python

# !pip install transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
import sys

# –ó–∞–º—ñ–Ω—ñ—Ç—å 'hf_xxx...' –Ω–∞ –≤–∞—à —Ä–µ–∞–ª—å–Ω–∏–π —Ç–æ–∫–µ–Ω –∑ https://huggingface.co/settings/tokens
token_var_name = "HF_TOKEN"
hf_token = os.getenv(token_var_name)
if hf_token is None:
    print(f"–ü–æ–º–∏–ª–∫–∞: –∑–º—ñ–Ω–Ω–∞ –æ—Ç–æ—á–µ–Ω–Ω—è '{token_var_name}' –Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–∞.", file=sys.stderr)
    sys.exit(1)

print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Ç–æ–∫–µ–Ω: {hf_token}")

model = "google/gemma-3-4b-it"

tokenizer = AutoTokenizer.from_pretrained(model, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model,
    token=hf_token,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

input_text = """–ü–µ—Ä–µ–∫–ª–∞–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∏–∂—á–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –≤–∏–≤–µ–¥–∏ –ª–∏—à–µ –ø–µ—Ä–µ–∫–ª–∞–¥. –û—Å—å —Ç–µ–∫—Å—Ç:
Ein Regionaljet mit 53 Passagieren und Crewmitgliedern schoss auf dem Roanoke-Blacksburg Regional Airport im amerikanischen Bundesstaat Virginia √ºber die Landebahn hinaus und kam nicht rechtzeitig zum Stehen. Zement-Zone stoppt Flieger."""

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
inputs = tokenizer(input_text, return_tensors="pt")

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ï –ø–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è –Ω–∞ GPU (—è–∫—â–æ —î CUDA)
if torch.cuda.is_available():
    inputs = inputs.to("cuda")
    model = model.to("cuda")

# # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,                    # ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π –¥–æ—Å—Ç—É–ø –¥–æ input_ids
        attention_mask=inputs.attention_mask, # ‚úÖ –î–æ–¥–∞—î–º–æ attention_mask
        max_new_tokens=300,                  # ‚úÖ –î–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É
        temperature=0.1,                     # ‚úÖ –ù–∏–∑—å–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç—ñ
        do_sample=True,                      # ‚úÖ –£–≤—ñ–º–∫–Ω—É—Ç–∏ —Å–µ–º–ø–ª—ñ–Ω–≥
        repetition_penalty=1.1,              # ‚úÖ –£–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–≤—Ç–æ—Ä—ñ–≤
        pad_token_id=tokenizer.eos_token_id, # ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π pad_token
        eos_token_id=tokenizer.eos_token_id  # ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π eos_token
    )

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ï –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è
full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("üì§ –ü–æ–≤–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:")
print(full_response)

print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")

# ========================================
# üí° –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–ò–ô –í–ê–†–Ü–ê–ù–¢ –ó –ö–†–ê–©–ò–ú –ü–†–û–ú–ü–¢–û–ú
# ========================================

print("\n" + "="*50)
print("üîÑ –ü—Ä–æ–±—É—î–º–æ –∑ —ñ–Ω—à–∏–º –ø—Ä–æ–º–ø—Ç–æ–º...")

# –ë—ñ–ª—å—à —á—ñ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç
better_prompt = f"""<start_of_turn>user
Translate this German text to Ukrainian:

{input_text.split('–û—Å—å —Ç–µ–∫—Å—Ç:')[-1].strip()}
<end_of_turn>
<start_of_turn>model
"""

inputs2 = tokenizer(better_prompt, return_tensors="pt")
if torch.cuda.is_available():
    inputs2 = inputs2.to("cuda")

with torch.no_grad():
    outputs2 = model.generate(
        inputs2.input_ids,
        max_new_tokens=300,
        temperature=0.1,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )

response2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
print("üì§ –í—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –Ω–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º:")
print(response2)
