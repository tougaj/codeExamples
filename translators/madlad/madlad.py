# -*- coding: utf-8 -*-
"""madlad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNfGaWzCA4s1fiAy_BMpgXYB397j3G5_
"""

# !pip uninstall -y transformers
!pip install transformers accelerate sentencepiece langdetect

# import transformers
# print(transformers.__version__)
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
from langdetect import detect

model_name = 'google/madlad400-3b-mt'
model = T5ForConditionalGeneration.from_pretrained(model_name, device_map="auto")
tokenizer = T5Tokenizer.from_pretrained(model_name)

def detect_language(text):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ —Ç–µ–∫—Å—Ç—É
    """
    try:
        detected_lang = detect(text)
        print(f"üîç –í–∏–∑–Ω–∞—á–µ–Ω–∞ –º–æ–≤–∞: {detected_lang}")
        return detected_lang
    except e:
        print("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –º–æ–≤—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º")
        return "en"

def translate_text(text, target_lang="uk", max_length=512, source_lang=None):
    """
    –ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –º–æ–≤–∏ —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏ –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ
    """
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–≤—É, —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–∞
    if source_lang is None:
        source_lang = detect_language(text)

    # –î–æ–¥–∞—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É
    formatted_text = f"<2{target_lang}> {text}"

    # –¢–æ–∫–µ–Ω—ñ–∑—É—î–º–æ —Ç–µ–∫—Å—Ç
    input_ids = tokenizer(
        formatted_text,
        return_tensors="pt",
        max_length=max_length,
        truncation=True,
        padding=True
    ).input_ids.to(model.device)

    print(f"üìù –î–æ–≤–∂–∏–Ω–∞ –≤—Ö—ñ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤: {input_ids.shape[1]}")

    # –ì–µ–Ω–µ—Ä—É—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥ –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            max_length=max_length,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –≤–∏—Ö–æ–¥—É
            min_length=10,  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞
            num_beams=4,    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è beam search –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ
            early_stopping=True,
            do_sample=False,  # –î–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏–π –≤–∏–≤—ñ–¥
            temperature=1.0,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # –î–µ–∫–æ–¥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result

def translate_long_text(text, target_lang="uk", chunk_size=400, source_lang=None):
    """
    –ü–µ—Ä–µ–∫–ª–∞–¥ –¥–æ–≤–≥–æ–≥–æ —Ç–µ–∫—Å—Ç—É –ø–æ —á–∞—Å—Ç–∏–Ω–∞—Ö –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –º–æ–≤–∏
    """
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–≤—É –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å—å–æ–≥–æ —Ç–µ–∫—Å—Ç—É
    if source_lang is None:
        source_lang = detect_language(text)

    # –†–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è
    sentences = text.split('. ')

    translated_parts = []
    current_chunk = ""

    for sentence in sentences:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–µ –ø–µ—Ä–µ–≤–∏—â–∏—Ç—å –Ω–∞—Å—Ç—É–ø–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ –ª—ñ–º—ñ—Ç
        test_chunk = current_chunk + sentence + ". "

        if len(tokenizer.encode(test_chunk)) < chunk_size:
            current_chunk = test_chunk
        else:
            # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É —á–∞—Å—Ç–∏–Ω—É
            if current_chunk.strip():
                print(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—é —á–∞—Å—Ç–∏–Ω—É: {current_chunk[:50]}...")
                translated = translate_text(current_chunk.strip(), target_lang, max_length=1024, source_lang=source_lang)
                translated_parts.append(translated)

            # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤—É —á–∞—Å—Ç–∏–Ω—É
            current_chunk = sentence + ". "

    # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é —á–∞—Å—Ç–∏–Ω—É
    if current_chunk.strip():
        print(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—é –æ—Å—Ç–∞–Ω–Ω—é —á–∞—Å—Ç–∏–Ω—É: {current_chunk[:50]}...")
        translated = translate_text(current_chunk.strip(), target_lang, max_length=1024, source_lang=source_lang)
        translated_parts.append(translated)

    return " ".join(translated_parts)

text = """<2uk> In seinem Post warf Trump Monaco vor, korrupt zu sein, ohne dies weiter zu begr√ºnden. Wegen ihrer angeblichen Verfehlungen habe die US-Regierung ihr zuletzt alle Sicherheitsfreigaben entzogen, ihr jeglichen Zugang zu nationalen Sicherheitsinformationen untersagt und ihr den Zutritt zu s√§mtlichen Bundesgeb√§uden verboten. Was genau sie falsch gemacht haben soll, sagte Trump nicht und lieferte auch keinerlei Hinweise.
Monaco war Sicherheitsberaterin in der Regierung des fr√ºheren demokratischen Pr√§sidenten Barack Obama und stellvertretende Justizministerin unter dem ehemaligen demokratischen Pr√§sidenten Joe Biden. Sie arbeitet seit Juli f√ºr Microsoft.
Trump setzt damit seine Vergeltungskampagne gegen seine politischen Gegner fort. Das US-Justizministerium hatte am Donnerstag Anklage gegen den fr√ºheren FBI-Direktor James Comey erhoben (mehr dazu lesen Sie hier ). Er hatte die Bundespolizei FBI geleitet, als diese eine Untersuchung der Verbindungen zwischen Trumps Wahlkampfteam 2016 und der russischen Regierung einleitete.
Comey werden Falschaussage und die Behinderung einer Untersuchung des Kongresses vorgeworfen. Bei einer Verurteilung drohen Comey bis zu f√ºnf Jahre Haft. Trump sagte am Freitag, er erwarte weitere Anklagen gegen von ihm als Feinde wahrgenommene Personen. ¬ªIch denke, es wird andere geben¬´, sagte er vor Reportern, f√ºgte jedoch hinzu, er habe keine Liste.
"""
# input_ids = tokenizer(text, return_tensors="pt").input_ids.to(model.device)
# outputs = model.generate(input_ids=input_ids)

# result = tokenizer.decode(outputs[0], skip_special_tokens=True)
# print(result)

print("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–µ—Ä–µ–∫–ª–∞–¥—É...")
print("üìú –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç:")
print(text)
print("\n" + "="*50 + "\n")

# –°–ø—Ä–æ–±—É—î–º–æ —Å–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ—Å—Ç–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥
print("üéØ –ü—Ä–æ—Å—Ç–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥:")
simple_result = translate_text(text, max_length=1024)
print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {simple_result}")

print("\n" + "="*50 + "\n")

# –Ø–∫—â–æ –ø—Ä–æ—Å—Ç–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –Ω–µ–ø–æ–≤–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥ –ø–æ —á–∞—Å—Ç–∏–Ω–∞—Ö
# print("üîß –ü–µ—Ä–µ–∫–ª–∞–¥ –ø–æ —á–∞—Å—Ç–∏–Ω–∞—Ö:")
# chunked_result = translate_long_text(text)
# print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {chunked_result}")