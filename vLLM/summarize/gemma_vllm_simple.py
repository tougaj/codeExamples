"""
–í–∞—Ä—ñ–∞–Ω—Ç A: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤–±—É–¥–æ–≤–∞–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ vLLM
–ü—Ä–æ—Å—Ç—ñ—à–∏–π –ø—ñ–¥—Ö—ñ–¥, —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –≤–∏–ø–∞–¥–∫—ñ–≤
"""

from vllm import LLM, SamplingParams
from typing import List

# üîß –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
llm = LLM(
    model="google/gemma-2-4b-it",  # –∞–±–æ —à–ª—è—Ö –¥–æ –ª–æ–∫–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ
    trust_remote_code=True,
    max_model_len=8192,  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    gpu_memory_utilization=0.9,  # –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU –ø–∞–º'—è—Ç—ñ
)

# üìù –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –∑ vLLM
tokenizer = llm.get_tokenizer()

# ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–ª—è summarization
sampling_params = SamplingParams(
    temperature=0.3,  # –Ω–∏–∑—å–∫–∞ –¥–ª—è –±—ñ–ª—å—à –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏—Ö —Ä–µ–∑—é–º–µ
    top_p=0.9,  # nucleus sampling
    max_tokens=200,  # –º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è —Ä–µ–∑—é–º–µ
    presence_penalty=0.5,  # –∑–Ω–∏–∂—É—î –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ–π
    frequency_penalty=0.3,  # –∑–Ω–∏–∂—É—î –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è —Å–ª—ñ–≤
    stop=["</summary>", "\n\n\n"],  # —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–∏
)


def prepare_chat_prompt(text: str) -> str:
    """
    –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º chat template
    """
    messages = [
        {
            "role": "user",
            "content": f"""–°—Ç–≤–æ—Ä–∏ —Å—Ç–∏—Å–ª–µ —Ä–µ–∑—é–º–µ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. 
–†–µ–∑—é–º–µ –º–∞—î –±—É—Ç–∏ —á—ñ—Ç–∫–∏–º, —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–º —Ç–∞ –Ω–µ –ø–µ—Ä–µ–≤–∏—â—É–≤–∞—Ç–∏ 3-4 —Ä–µ—á–µ–Ω–Ω—è.

–¢–µ–∫—Å—Ç:
{text}

–†–µ–∑—é–º–µ:"""
        }
    ]
    
    # üéØ –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è chat template
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,  # –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ string, –Ω–µ —Ç–æ–∫–µ–Ω–∏
        add_generation_prompt=True  # –¥–æ–¥–∞—î prompt –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    )
    
    return prompt


def summarize_texts(texts: List[str]) -> List[str]:
    """
    Batch-–æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∑—é–º–µ
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è summarization
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö —Ä–µ–∑—é–º–µ
    """
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å—ñ—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤
    prompts = [prepare_chat_prompt(text) for text in texts]
    
    # üöÄ Batch inference
    outputs = llm.generate(prompts, sampling_params)
    
    # Extraction —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    summaries = [output.outputs[0].text.strip() for output in outputs]
    
    return summaries


# üìä –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ñ —Ç–µ–∫—Å—Ç–∏
    sample_texts = [
        """–®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç (–®–Ü) ‚Äî —Ü–µ –≥–∞–ª—É–∑—å –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–∏—Ö –Ω–∞—É–∫, —è–∫–∞ –∑–∞–π–º–∞—î—Ç—å—Å—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è–º 
        —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏—Ö –º–∞—à–∏–Ω, –∑–¥–∞—Ç–Ω–∏—Ö –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –∑–∞–≤–¥–∞–Ω–Ω—è, —â–æ –∑–∞–∑–≤–∏—á–∞–π –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –ª—é–¥—Å—å–∫–æ–≥–æ 
        —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É. –¶–µ –≤–∫–ª—é—á–∞—î —Ç–∞–∫—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ, —è–∫ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏, –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å, 
        –ø–µ—Ä–µ–∫–ª–∞–¥ –º–æ–≤ —Ç–∞ –≤—ñ–∑—É–∞–ª—å–Ω–µ —Å–ø—Ä–∏–π–Ω—è—Ç—Ç—è. –°—É—á–∞—Å–Ω–∏–π –®–Ü –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è 
        —Ç–∞ –≥–ª–∏–±–æ–∫—ñ –Ω–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –≤—Ä–∞–∂–∞—é—á–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É —Ä—ñ–∑–Ω–∏—Ö —Å—Ñ–µ—Ä–∞—Ö.""",
        
        """Python —î –æ–¥–Ω—ñ—î—é –∑ –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏—Ö –º–æ–≤ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —É —Å–≤—ñ—Ç—ñ. –í–æ–Ω–∞ –≤—ñ–¥–æ–º–∞ —Å–≤–æ—î—é 
        –ø—Ä–æ—Å—Ç–æ—Ç–æ—é —Ç–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—é —Å–∏–Ω—Ç–∞–∫—Å–∏—Å—É, —â–æ —Ä–æ–±–∏—Ç—å —ó—ó —ñ–¥–µ–∞–ª—å–Ω–æ—é –¥–ª—è –ø–æ—á–∞—Ç–∫—ñ–≤—Ü—ñ–≤. 
        Python —à–∏—Ä–æ–∫–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ –≤–µ–±-—Ä–æ–∑—Ä–æ–±—Ü—ñ, –Ω–∞—É–∫–æ–≤–æ–º—É –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—ñ, –∞–Ω–∞–ª—ñ–∑—ñ 
        –¥–∞–Ω–∏—Ö, —à—Ç—É—á–Ω–æ–º—É —ñ–Ω—Ç–µ–ª–µ–∫—Ç—ñ —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó. –í–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –±—ñ–±–ª—ñ–æ—Ç–µ–∫ —Ç–∞ –∞–∫—Ç–∏–≤–Ω–∞ 
        —Å–ø—ñ–ª—å–Ω–æ—Ç–∞ —Ä–æ–±–ª—è—Ç—å Python —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–º —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤.""",
    ]
    
    print("üîÑ –ü–æ—á–∞—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ä–µ–∑—é–º–µ...\n")
    
    summaries = summarize_texts(sample_texts)
    
    # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    for i, (original, summary) in enumerate(zip(sample_texts, summaries), 1):
        print(f"üìÑ –¢–µ–∫—Å—Ç #{i}:")
        print(f"{original[:100]}...")
        print(f"\n‚ú® –†–µ–∑—é–º–µ:")
        print(f"{summary}")
        print("\n" + "="*80 + "\n")
    
    # üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"‚úÖ –û–±—Ä–æ–±–ª–µ–Ω–æ —Ç–µ–∫—Å—Ç—ñ–≤: {len(sample_texts)}")
    print(f"üìä –°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ —Ä–µ–∑—é–º–µ: {sum(len(s.split()) for s in summaries) / len(summaries):.1f} —Å–ª—ñ–≤")
