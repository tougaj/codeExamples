"""
üöÄ FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ vLLM –º–æ–¥–µ–ª—è–º–∏
–ü—ñ–¥—Ç—Ä–∏–º—É—î –ø–µ—Ä–µ–∫–ª–∞–¥, —Ä–µ–∑—é–º–µ —Ç–∞ –¥–æ–≤—ñ–ª—å–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é —Ç–µ–∫—Å—Ç—É
"""

import os
import time
from contextlib import asynccontextmanager
from typing import List, Optional

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, status
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, field_validator
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö –æ—Ç–æ—á–µ–Ω–Ω—è
load_dotenv()

# üîß –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª—ñ —Ç–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
llm: Optional[LLM] = None
tokenizer = None


# üìã Pydantic –º–æ–¥–µ–ª—ñ –¥–ª—è –∑–∞–ø–∏—Ç—ñ–≤ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
class SamplingParamsRequest(BaseModel):
    """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É"""

    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    top_p: float = Field(default=0.9, ge=0.0, le=1.0)
    max_tokens: int = Field(default=512, ge=1, le=8192)
    presence_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
    frequency_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
    stop: Optional[List[str]] = Field(default=None)


class TextRequest(BaseModel):
    """–ó–∞–ø–∏—Ç –∑ —Ç–µ–∫—Å—Ç–∞–º–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏"""

    texts: List[str] = Field(..., min_length=1, max_length=100)
    sampling_params: Optional[SamplingParamsRequest] = None

    @field_validator("texts")
    @classmethod
    def validate_texts(cls, v: List[str]) -> List[str]:
        if not v:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        for text in v:
            if not text.strip():
                raise ValueError("–¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        return v


class ChatRequest(BaseModel):
    """–ó–∞–ø–∏—Ç –¥–ª—è —á–∞—Ç-–≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑ –ø—Ä–æ–º–ø—Ç–æ–º"""

    texts: List[str] = Field(..., min_length=1, max_length=100)
    sampling_params: Optional[SamplingParamsRequest] = None

    @field_validator("texts")
    @classmethod
    def validate_texts(cls, v: List[str]) -> List[str]:
        if not v:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        for text in v:
            if not text.strip():
                raise ValueError("–¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        return v


class RawTextRequest(BaseModel):
    """–ó–∞–ø–∏—Ç –¥–ª—è –ø—Ä—è–º–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –±–µ–∑ chat template"""

    texts: List[str] = Field(..., min_length=1, max_length=100)
    sampling_params: Optional[SamplingParamsRequest] = None

    @field_validator("texts")
    @classmethod
    def validate_texts(cls, v: List[str]) -> List[str]:
        if not v:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        for text in v:
            if not text.strip():
                raise ValueError("–¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        return v


class GenerationResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É"""

    text: str
    finish_reason: Optional[str]
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None


class BatchResponse(BaseModel):
    """–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ batch –∑–∞–ø–∏—Ç"""

    results: List[GenerationResult]
    total_texts: int
    processing_time: float
    avg_input_tokens: Optional[float] = None
    avg_output_tokens: Optional[float] = None
    total_input_tokens: Optional[int] = None
    total_output_tokens: Optional[int] = None


class HealthResponse(BaseModel):
    """–í—ñ–¥–ø–æ–≤—ñ–¥—å health check"""

    status: str
    model_loaded: bool
    model_name: Optional[str] = None
    max_model_len: Optional[int] = None
    gpu_memory_utilization: Optional[float] = None


# üéØ –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –ø—Ä–æ–º–ø—Ç–∞–º–∏
def prepare_translation_prompt(text: str) -> str:
    """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—É –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É"""
    messages = [
        {
            "role": "user",
            "content": f"""–¢–∏ ‚Äî –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥–∞—á —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏.
–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–∏ –æ—Ç—Ä–∏–º–∞–Ω–∏–π —Ç–µ–∫—Å—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é **—Ç–æ—á–Ω–æ –∑–∞ –∑–º—ñ—Å—Ç–æ–º**, –∞–ª–µ **–ø—Ä–∏—Ä–æ–¥–Ω–æ –π –≤–∏—Ä–∞–∑–Ω–æ –∑–∞ —Ñ–æ—Ä–º–æ—é**, –¥–æ—Ç—Ä–∏–º—É—é—á–∏—Å—å —Ç–∞–∫–∏—Ö –ø—Ä–∞–≤–∏–ª:
1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **–≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É, –ø—Ä–∏—Ä–æ–¥–Ω—É —Ç–∞ —Å—Ç–∏–ª—ñ—Å—Ç–∏—á–Ω–æ –¥–æ—Ä–µ—á–Ω—É** —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É.
2. **–ù–µ –ø–µ—Ä–µ–∫–ª–∞–¥–∞–π –¥–æ—Å–ª—ñ–≤–Ω–æ.** –£–Ω–∏–∫–∞–π –∫–∞–ª—å–æ–∫, —à—Ç—É—á–Ω–∏—Ö –∑–≤–æ—Ä–æ—Ç—ñ–≤ —ñ –±—É–∫–≤–∞–ª—å–Ω–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π ‚Äî –∑–∞–º—ñ–Ω—é–π —ó—Ö –Ω–∞ –ø—Ä–∏—Ä–æ–¥–Ω—ñ —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–∫–∏ –∞–±–æ —ñ–¥—ñ–æ–º–∞—Ç–∏—á–Ω—ñ –≤–∏—Ä–∞–∑–∏.
3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –ª–∞–ø–∫–∏ **¬´...¬ª** –∑–∞–º—ñ—Å—Ç—å —ñ–Ω–æ–∑–µ–º–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ (‚Äû...", "...", '...' —Ç–æ—â–æ), –¥–æ—Ç—Ä–∏–º—É—é—á–∏—Å—å –ø—Ä–∞–≤–∏–ª –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó.
4. –ó–∞ –ø–æ—Ç—Ä–µ–±–∏ –∑–∞—Å—Ç–æ—Å–æ–≤—É–π **—Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è Markdown**:
   * –∑–∞–≥–æ–ª–æ–≤–∫–∏ (`#`, `##`),
   * —Å–ø–∏—Å–∫–∏,
   * **–∂–∏—Ä–Ω–∏–π** –∞–±–æ *–∫—É—Ä—Å–∏–≤–Ω–∏–π* —Ç–µ–∫—Å—Ç,
   * **–∂–∏—Ä–Ω–∏–π** —Ç–µ–∫—Å—Ç –¥–ª—è —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π (NER),
   * —Ü–∏—Ç–∞—Ç–∏ —Ç–æ—â–æ.
5. –ù–µ –¥–æ–¥–∞–≤–∞–π –ø–æ—è—Å–Ω–µ–Ω—å, –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, –ø—Ä–∏–º—ñ—Ç–æ–∫ —á–∏ —Å–ª—É–∂–±–æ–≤–∏—Ö —Ñ—Ä–∞–∑.
6. –Ø–∫—â–æ –Ω–∞–¥–∞–Ω–æ —Ç–µ–∫—Å—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, –∞–±–æ —Ä–æ—Å—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é, —Ç–æ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—Ç–∏ –π–æ–≥–æ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ. –í —Ç–∞–∫–æ–º—É —Ä–∞–∑—ñ –ø—Ä–æ—Å—Ç–æ –≤–∏–≤–µ–¥–∏ –ø—É—Å—Ç–∏–π —Ä—è–¥–æ–∫.
7. **–£ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –ø–æ–¥–∞–≤–∞–π –ª–∏—à–µ –ø–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–∏–π —Ç–µ–∫—Å—Ç.**
–¢–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É:\n\n{text}""",
        }
    ]

    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )


def prepare_summary_prompt(text: str) -> str:
    """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—É –¥–ª—è —Ä–µ–∑—é–º–µ"""
    messages = [
        {
            "role": "user",
            "content": f"""–¢–∏ ‚Äî —Å–∏—Å—Ç–µ–º–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç–∏—Å–ª–∏—Ö –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Ä–µ–∑—é–º–µ.
–û—Ç—Ä–∏–º—É—î—à —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ –∑ –Ω–æ–≤–∏–Ω–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞ –±—É–¥—å-—è–∫–æ—é –º–æ–≤–æ—é.
–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –∫–æ—Ä–æ—Ç–∫–µ —Ä–µ–∑—é–º–µ **—É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é** (3‚Äì5 —Ä–µ—á–µ–Ω—å), –¥–æ—Ç—Ä–∏–º—É—é—á–∏—Å—å —Ç–∞–∫–∏—Ö –ø—Ä–∞–≤–∏–ª:
1. –†–µ–∑—é–º–µ –º–∞—î **–æ–±–æ–≤'—è–∑–∫–æ–≤–æ** –±—É—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é!.
2. **–ü–µ—Ä–µ–¥–∞–π –≥–æ–ª–æ–≤–Ω–∏–π –∑–º—ñ—Å—Ç —Ç–æ—á–Ω–æ –π —Å—Ç–∏—Å–ª–æ** ‚Äî –∑–æ—Å–µ—Ä–µ–¥—å—Å—è –Ω–∞ –∫–ª—é—á–æ–≤–∏—Ö —Ñ–∞–∫—Ç–∞—Ö, –ø–æ–¥—ñ—è—Ö —ñ —Ç–µ–∑–∞—Ö.
3. –Ø–∫—â–æ –≤ —Ç–µ–∫—Å—Ç—ñ —î —Ü—ñ –¥–∞–Ω—ñ, **–∑–∞–∑–Ω–∞—á –æ—Å–Ω–æ–≤–Ω–∏—Ö —É—á–∞—Å–Ω–∏–∫—ñ–≤, –º—ñ—Å—Ü–µ, —á–∞—Å —ñ –ø—Ä–∏—á–∏–Ω—É –ø–æ–¥—ñ—ó.**
4. **–£–Ω–∏–∫–∞–π** –¥—Ä—É–≥–æ—Ä—è–¥–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π, –ø—Ä–∏–∫–ª–∞–¥—ñ–≤, —Ü–∏—Ç–∞—Ç, –æ—Ü—ñ–Ω–Ω–∏—Ö —Å—É–¥–∂–µ–Ω—å —ñ –µ–º–æ—Ü—ñ–π–Ω–æ–≥–æ —Ç–æ–Ω—É.
5. –î–æ—Ç—Ä–∏–º—É–π—Å—è **–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–≥–æ, –æ–±'—î–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ —Å—Ç–∏–ª—é.**
6. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **–ø—Ä–∏—Ä–æ–¥–Ω—É, –∑—Ä–æ–∑—É–º—ñ–ª—É –π –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É** —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É.
7. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Markdown **–∂–∏—Ä–Ω–∏–π** —Ç–µ–∫—Å—Ç –¥–ª—è —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π (NER).
8. **–£ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –ø–æ–¥–∞–π –ª–∏—à–µ —Ä–µ–∑—é–º–µ** ‚Äî –±–µ–∑ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, –ø–æ—è—Å–Ω–µ–Ω—å, –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –∞–±–æ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è.
–¢–µ–∫—Å—Ç –¥–ª—è —Ä–µ–∑—é–º–µ:\n\n{text}""",
        }
    ]

    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )


def prepare_chat_prompt(text: str) -> str:
    """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—É –¥–ª—è –¥–æ–≤—ñ–ª—å–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó"""
    messages = [
        {
            "role": "user",
            "content": text,
        }
    ]

    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )


def validate_text_length(text: str, max_model_len: int) -> None:
    """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–æ–≤–∂–∏–Ω–∏ —Ç–µ–∫—Å—Ç—É"""
    # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∏
    estimated_tokens = len(text) / 4

    if estimated_tokens > max_model_len * 0.8:  # –∑–∞–ª–∏—à–∞—î–º–æ 20% –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        raise ValueError(
            f"–¢–µ–∫—Å—Ç –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π. –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤: {int(estimated_tokens)}, "
            f"–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ: {int(max_model_len * 0.8)}"
        )


def create_sampling_params(params: Optional[SamplingParamsRequest]) -> SamplingParams:
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è SamplingParams –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
    if params is None:
        params = SamplingParamsRequest()

    return SamplingParams(
        temperature=params.temperature,
        top_p=params.top_p,
        max_tokens=params.max_tokens,
        presence_penalty=params.presence_penalty,
        frequency_penalty=params.frequency_penalty,
        stop=params.stop,
    )


async def generate_texts(
    texts: List[str], sampling_params: SamplingParams, max_model_len: int
) -> BatchResponse:
    """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—ñ–≤ —á–µ—Ä–µ–∑ vLLM"""
    start_time = time.time()

    try:
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–æ–≤–∂–∏–Ω–∏ –∫–æ–∂–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        for i, text in enumerate(texts):
            try:
                validate_text_length(text, max_model_len)
            except ValueError as e:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"–ü–æ–º–∏–ª–∫–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É #{i + 1}: {str(e)}",
                )

        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è
        outputs = llm.generate(texts, sampling_params)

        # –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        results = []
        total_input_tokens = 0
        total_output_tokens = 0

        for output in outputs:
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤ –∑ –º–µ—Ç—Ä–∏–∫
            prompt_tokens = len(output.prompt_token_ids) if output.prompt_token_ids else None
            completion_tokens = (
                len(output.outputs[0].token_ids) if output.outputs[0].token_ids else None
            )

            if prompt_tokens is not None:
                total_input_tokens += prompt_tokens
            if completion_tokens is not None:
                total_output_tokens += completion_tokens

            results.append(
                GenerationResult(
                    text=output.outputs[0].text.strip(),
                    finish_reason=output.outputs[0].finish_reason,
                    input_tokens=prompt_tokens,
                    output_tokens=completion_tokens,
                )
            )

        processing_time = time.time() - start_time

        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å
        valid_input_tokens = [r.input_tokens for r in results if r.input_tokens is not None]
        valid_output_tokens = [
            r.output_tokens for r in results if r.output_tokens is not None
        ]

        return BatchResponse(
            results=results,
            total_texts=len(texts),
            processing_time=round(processing_time, 4),
            avg_input_tokens=(
                round(sum(valid_input_tokens) / len(valid_input_tokens), 2)
                if valid_input_tokens
                else None
            ),
            avg_output_tokens=(
                round(sum(valid_output_tokens) / len(valid_output_tokens), 2)
                if valid_output_tokens
                else None
            ),
            total_input_tokens=total_input_tokens if total_input_tokens > 0 else None,
            total_output_tokens=total_output_tokens if total_output_tokens > 0 else None,
        )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(e)}",
        )


# üé¨ Lifecycle —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è
@asynccontextmanager
async def lifespan(app: FastAPI):
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤"""
    global llm, tokenizer

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∑ .env
    MODEL_NAME = os.getenv("MODEL_NAME", "google/gemma-3-4b-it")
    MAX_MODEL_LEN = int(os.getenv("MAX_MODEL_LEN", "8192"))
    GPU_MEMORY_UTILIZATION = float(os.getenv("GPU_MEMORY_UTILIZATION", "0.9"))
    TENSOR_PARALLEL_SIZE = int(os.getenv("TENSOR_PARALLEL_SIZE", "1"))
    MAX_NUM_SEQS = int(os.getenv("MAX_NUM_SEQS", "15"))
    SWAP_SPACE = int(os.getenv("SWAP_SPACE", "8"))
    ENABLE_PREFIX_CACHING = os.getenv("ENABLE_PREFIX_CACHING", "true").lower() == "true"
    ENABLE_CHUNKED_PREFILL = (
        os.getenv("ENABLE_CHUNKED_PREFILL", "true").lower() == "true"
    )
    DTYPE = os.getenv("DTYPE", "auto")

    print("üîß –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è vLLM –º–æ–¥–µ–ª—ñ...")
    print(f"üì¶ –ú–æ–¥–µ–ª—å: {MODEL_NAME}")

    try:
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        llm = LLM(
            model=MODEL_NAME,
            max_model_len=MAX_MODEL_LEN,
            gpu_memory_utilization=GPU_MEMORY_UTILIZATION,
            tensor_parallel_size=TENSOR_PARALLEL_SIZE,
            max_num_seqs=MAX_NUM_SEQS,
            swap_space=SWAP_SPACE,
            enable_prefix_caching=ENABLE_PREFIX_CACHING,
            enable_chunked_prefill=ENABLE_CHUNKED_PREFILL,
            dtype=DTYPE,
        )

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞!")

    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        raise

    yield

    print("üîÑ –í–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")


# üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è FastAPI
app = FastAPI(
    title="vLLM API Server",
    description="API –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ vLLM –º–æ–¥–µ–ª—è–º–∏: –ø–µ—Ä–µ–∫–ª–∞–¥, —Ä–µ–∑—é–º–µ, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è",
    version="1.0.0",
    lifespan=lifespan,
)


# üìç –ï–Ω–¥–ø–æ—ñ–Ω—Ç–∏
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É —Å–µ—Ä–≤–µ—Ä–∞"""
    if llm is None:
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"status": "unhealthy", "model_loaded": False},
        )

    return HealthResponse(
        status="healthy",
        model_loaded=True,
        model_name=os.getenv("MODEL_NAME", "google/gemma-3-4b-it"),
        max_model_len=int(os.getenv("MAX_MODEL_LEN", "8192")),
        gpu_memory_utilization=float(os.getenv("GPU_MEMORY_UTILIZATION", "0.9")),
    )


@app.post("/translate", response_model=BatchResponse)
async def translate(request: TextRequest):
    """–ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—ñ–≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é"""
    if llm is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞",
        )

    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É
    prompts = [prepare_translation_prompt(text) for text in request.texts]

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É
    if request.sampling_params is None:
        request.sampling_params = SamplingParamsRequest(
            temperature=0.1, top_p=0.9, max_tokens=8192
        )

    sampling_params = create_sampling_params(request.sampling_params)
    max_model_len = int(os.getenv("MAX_MODEL_LEN", "8192"))

    return await generate_texts(prompts, sampling_params, max_model_len)


@app.post("/summarize", response_model=BatchResponse)
async def summarize(request: TextRequest):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç—ñ–≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é"""
    if llm is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞",
        )

    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—ñ–≤ –¥–ª—è —Ä–µ–∑—é–º–µ
    prompts = [prepare_summary_prompt(text) for text in request.texts]

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º –¥–ª—è —Ä–µ–∑—é–º–µ
    if request.sampling_params is None:
        request.sampling_params = SamplingParamsRequest(
            temperature=0.5,
            top_p=0.9,
            max_tokens=400,
            presence_penalty=0.5,
            frequency_penalty=0.3,
        )

    sampling_params = create_sampling_params(request.sampling_params)
    max_model_len = int(os.getenv("MAX_MODEL_LEN", "8192"))

    return await generate_texts(prompts, sampling_params, max_model_len)


@app.post("/generate", response_model=BatchResponse)
async def generate(request: ChatRequest):
    """–î–æ–≤—ñ–ª—å–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º chat template"""
    if llm is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞",
        )

    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç—ñ–≤ —á–µ—Ä–µ–∑ chat template
    prompts = [prepare_chat_prompt(text) for text in request.texts]

    sampling_params = create_sampling_params(request.sampling_params)
    max_model_len = int(os.getenv("MAX_MODEL_LEN", "8192"))

    return await generate_texts(prompts, sampling_params, max_model_len)


@app.post("/generate-raw", response_model=BatchResponse)
async def generate_raw(request: RawTextRequest):
    """–ü—Ä—è–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –±–µ–∑ chat template (—Å–∏—Ä–∏–π —Ç–µ–∫—Å—Ç)"""
    if llm is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞",
        )

    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –Ω–∞–ø—Ä—è–º—É, –±–µ–∑ chat template
    prompts = request.texts

    sampling_params = create_sampling_params(request.sampling_params)
    max_model_len = int(os.getenv("MAX_MODEL_LEN", "8192"))

    return await generate_texts(prompts, sampling_params, max_model_len)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host=os.getenv("HOST", "0.0.0.0"),
        port=int(os.getenv("PORT", "8000")),
        reload=os.getenv("RELOAD", "false").lower() == "true",
    )
