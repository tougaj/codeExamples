# üöÄ vLLM FastAPI Server

FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ vLLM –º–æ–¥–µ–ª—è–º–∏ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –ø–µ—Ä–µ–∫–ª–∞–¥—É, —Ä–µ–∑—é–º–µ —Ç–∞ –¥–æ–≤—ñ–ª—å–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É.

## üìã –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ

- ‚úÖ Batch inference –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏
- üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—ñ–≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é
- üìù –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∑—é–º–µ –Ω–æ–≤–∏–Ω
- üí¨ –î–æ–≤—ñ–ª—å–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ chat template
- üìä –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤
- üè• Health check –µ–Ω–¥–ø–æ—ñ–Ω—Ç
- ‚ö° –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ GPU –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è–º–∏ vLLM

## üõ†Ô∏è –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è

### 1. –ö–ª–æ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π

```bash
# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ—á–µ–Ω–Ω—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
python -m venv venv
source venv/bin/activate  # –¥–ª—è Ubuntu

# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π
pip install -r requirements.txt
```

### 2. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó

–°—Ç–≤–æ—Ä–∏ —Ñ–∞–π–ª `.env` –Ω–∞ –æ—Å–Ω–æ–≤—ñ `.env.example`:

```bash
cp .env.example .env
```

–í—ñ–¥—Ä–µ–¥–∞–≥—É–π `.env` –ø—ñ–¥ —Å–≤–æ—ó –ø–æ—Ç—Ä–µ–±–∏:

```env
MODEL_NAME=google/gemma-3-4b-it
MAX_MODEL_LEN=8192
GPU_MEMORY_UTILIZATION=0.9
PORT=8000
```

## üöÄ –ó–∞–ø—É—Å–∫

### Developer —Ä–µ–∂–∏–º (–∑ auto-reload)

```bash
# –í—Å—Ç–∞–Ω–æ–≤–∏ RELOAD=true –≤ .env –∞–±–æ:
RELOAD=true python main.py

# –ê–±–æ —á–µ—Ä–µ–∑ uvicorn –Ω–∞–ø—Ä—è–º—É:
uv run uvicorn fastAPI.main:app --reload --host 0.0.0.0 --port 8000
```

### Production —Ä–µ–∂–∏–º

```bash
# –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Python
python main.py

# –ê–±–æ —á–µ—Ä–µ–∑ uvicorn –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è–º–∏:
uvicorn main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 1 \
  --log-level info
```

**‚ö†Ô∏è –ü—Ä–∏–º—ñ—Ç–∫–∞:** –î–ª—è production –∫—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ `--workers 1`, –æ—Å–∫—ñ–ª—å–∫–∏ vLLM –º–æ–¥–µ–ª—å –∑–∞–π–º–∞—î –±–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ GPU. –Ø–∫—â–æ —Ç—Ä–µ–±–∞ –±—ñ–ª—å—à–µ workers, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π load balancer –∑ –¥–µ–∫—ñ–ª—å–∫–æ–º–∞ —ñ–Ω—Å—Ç–∞–Ω—Å–∞–º–∏.

### Production –∑ Gunicorn + Uvicorn workers

```bash
pip install gunicorn

gunicorn main:app \
  --workers 1 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 300 \
  --log-level info
```

## üìç API –ï–Ω–¥–ø–æ—ñ–Ω—Ç–∏

### Health Check

```bash
GET /health
```

**–í—ñ–¥–ø–æ–≤—ñ–¥—å:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_name": "google/gemma-3-4b-it",
  "max_model_len": 8192,
  "gpu_memory_utilization": 0.9
}
```

### 1. –ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—ñ–≤

```bash
POST /translate
```

**–ó–∞–ø–∏—Ç:**
```json
{
  "texts": [
    "Hello world! This is a test.",
    "Machine learning is amazing."
  ],
  "sampling_params": {
    "temperature": 0.1,
    "top_p": 0.9,
    "max_tokens": 8192
  }
}
```

**–í—ñ–¥–ø–æ–≤—ñ–¥—å:**
```json
{
  "results": [
    {
      "text": "–ü—Ä–∏–≤—ñ—Ç, —Å–≤—ñ—Ç! –¶–µ —Ç–µ—Å—Ç.",
      "finish_reason": "stop",
      "input_tokens": 45,
      "output_tokens": 12
    }
  ],
  "total_texts": 2,
  "processing_time": 1.2345,
  "avg_input_tokens": 42.5,
  "avg_output_tokens": 10.5,
  "total_input_tokens": 85,
  "total_output_tokens": 21
}
```

### 2. –†–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç—ñ–≤

```bash
POST /summarize
```

**–ó–∞–ø–∏—Ç:**
```json
{
  "texts": [
    "Long article text here..."
  ],
  "sampling_params": {
    "temperature": 0.5,
    "max_tokens": 400
  }
}
```

### 3. –î–æ–≤—ñ–ª—å–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è (–∑ chat template)

```bash
POST /generate
```

**–ó–∞–ø–∏—Ç:**
```json
{
  "texts": [
    "–†–æ–∑–∫–∞–∂–∏ –º–µ–Ω—ñ –ø—Ä–æ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç"
  ],
  "sampling_params": {
    "temperature": 0.7,
    "max_tokens": 512
  }
}
```

### 4. –ü—Ä—è–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è (–±–µ–∑ chat template)

```bash
POST /generate-raw
```

**–ó–∞–ø–∏—Ç:**
```json
{
  "texts": [
    "Complete this sentence: The future of AI is"
  ],
  "sampling_params": {
    "temperature": 0.8
  }
}
```

## üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è API

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è curl

```bash
# Health check
curl http://localhost:8000/health

# –ü–µ—Ä–µ–∫–ª–∞–¥
curl -X POST http://localhost:8000/translate \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Hello, how are you?"],
    "sampling_params": {
      "temperature": 0.1,
      "max_tokens": 100
    }
  }'

# –†–µ–∑—é–º–µ
curl -X POST http://localhost:8000/summarize \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Your long text here..."]
  }'
```

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Python

```python
import requests

url = "http://localhost:8000/translate"
data = {
    "texts": ["Hello world!"],
    "sampling_params": {
        "temperature": 0.1,
        "max_tokens": 100
    }
}

response = requests.post(url, json=data)
print(response.json())
```

## üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (sampling_params)

–í—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ, –º–∞—é—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º:

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º | –î—ñ–∞–ø–∞–∑–æ–Ω | –û–ø–∏—Å |
|----------|---------------|----------|------|
| `temperature` | 0.7 | 0.0 - 2.0 | –ö—Ä–µ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å (‚Üë = –±—ñ–ª—å—à–µ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ) |
| `top_p` | 0.9 | 0.0 - 1.0 | Nucleus sampling |
| `max_tokens` | 512 | 1 - 8192 | –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω—ñ–≤ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ |
| `presence_penalty` | 0.0 | -2.0 - 2.0 | –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ–π |
| `frequency_penalty` | 0.0 | -2.0 - 2.0 | –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è —Å–ª—ñ–≤ |
| `stop` | null | - | –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ñ–≤ |

## ‚öôÔ∏è –ó–º—ñ–Ω–Ω—ñ –æ—Ç–æ—á–µ–Ω–Ω—è (.env)

| –ó–º—ñ–Ω–Ω–∞ | –ó–∞ –∑–∞–º–æ–≤—á–∞–Ω–Ω—è–º | –û–ø–∏—Å |
|--------|---------------|------|
| `MODEL_NAME` | google/gemma-3-4b-it | –ù–∞–∑–≤–∞ –∞–±–æ —à–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ |
| `MAX_MODEL_LEN` | 8192 | –ú–∞–∫—Å. –¥–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É |