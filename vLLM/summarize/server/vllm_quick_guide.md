# üöÄ Quick Start Guide: vLLM API –¥–ª—è Gemma 3

## üì¶ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π

```bash
# vLLM (—è–∫—â–æ —â–µ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)
pip install vllm

# OpenAI Python SDK –¥–ª—è –∫–ª—ñ—î–Ω—Ç–∞
pip install openai

# –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ: –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
pip install httpx
```

## üé¨ –ö—Ä–æ–∫ 1: –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞

### –í–∞—Ä—ñ–∞–Ω—Ç A: –ß–µ—Ä–µ–∑ bash —Å–∫—Ä–∏–ø—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
```bash
# –ó—Ä–æ–±–∏ —Å–∫—Ä–∏–ø—Ç –≤–∏–∫–æ–Ω—É–≤–∞–Ω–∏–º
chmod +x start_vllm_server.sh

# –ó–∞–ø—É—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä
./start_vllm_server.sh
```

### –í–∞—Ä—ñ–∞–Ω—Ç B: –ü—Ä—è–º–∞ –∫–æ–º–∞–Ω–¥–∞
```bash
vllm serve google/gemma-2-4b-it \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```

### –í–∞—Ä—ñ–∞–Ω—Ç C: –ó –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—î—é
```bash
vllm serve google/gemma-2-4b-it \
    --host 0.0.0.0 \
    --port 8000 \
    --api-key "your-secret-api-key-here" \
    --trust-remote-code
```

**–°–µ—Ä–≤–µ—Ä –±—É–¥–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π –Ω–∞:**
- API: `http://localhost:8000/v1`
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è: `http://localhost:8000/docs`
- Health check: `http://localhost:8000/health`

---

## üß™ –ö—Ä–æ–∫ 2: –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ curl

### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è
```bash
curl http://localhost:8000/health
```

### –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
```bash
curl http://localhost:8000/v1/models
```

### –ü—Ä–æ—Å—Ç–∏–π –∑–∞–ø–∏—Ç –Ω–∞ summarization
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/gemma-2-4b-it",
    "messages": [
      {
        "role": "user",
        "content": "–°—Ç–≤–æ—Ä–∏ –∫–æ—Ä–æ—Ç–∫–µ —Ä–µ–∑—é–º–µ: Python - —Ü–µ –º–æ–≤–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –≤–∏—Å–æ–∫–æ–≥–æ —Ä—ñ–≤–Ω—è."
      }
    ],
    "temperature": 0.5,
    "max_tokens": 100
  }'
```

### –ó streaming
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/gemma-2-4b-it",
    "messages": [{"role": "user", "content": "–†–µ–∑—é–º–µ –ø—Ä–æ Python"}],
    "stream": true
  }'
```

---

## üêç –ö—Ä–æ–∫ 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Python –∫–ª—ñ—î–Ω—Ç–∞

### –ë–∞–∑–æ–≤–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
```python
from vllm_api_client import VLLMSummarizerClient

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
client = VLLMSummarizerClient()

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è
client.check_health()

# –û–¥–∏–Ω–æ—á–Ω–µ summarization
result = client.summarize_single(
    text="–í–∞—à —Ç–µ–∫—Å—Ç —Ç—É—Ç...",
    config="balanced"
)
print(result['summary'])
```

### Batch-–æ–±—Ä–æ–±–∫–∞
```python
texts = ["—Ç–µ–∫—Å—Ç 1", "—Ç–µ–∫—Å—Ç 2", "—Ç–µ–∫—Å—Ç 3"]
results = client.summarize_batch(texts, config="precise")

for r in results:
    print(r['summary'])
```

### Streaming —Ä–µ–∂–∏–º
```python
result = client.summarize_single(
    text="–í–∞—à —Ç–µ–∫—Å—Ç...",
    stream=True  # –í–∏–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç—É–ø–æ–≤–æ
)
```

---

## ‚öôÔ∏è –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó

### Precise (—Ç–æ—á–Ω–µ —Ä–µ–∑—é–º–µ)
- Temperature: 0.2 (–º–∞–ª–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
- Max tokens: 150
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–ª—è: —Ñ–∞–∫—Ç–∏—á–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤, –Ω–æ–≤–∏–Ω

### Balanced (–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–µ)
- Temperature: 0.5
- Max tokens: 200
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–ª—è: –∑–∞–≥–∞–ª—å–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤

### Creative (–∫—Ä–µ–∞—Ç–∏–≤–Ω–µ)
- Temperature: 0.8
- Max tokens: 250
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–ª—è: —Ö—É–¥–æ–∂–Ω—ñ—Ö —Ç–µ–∫—Å—Ç—ñ–≤, –µ—Å–µ—ó–≤

---

## üîß –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É

### Systemd service (–∞–≤—Ç–æ–∑–∞–ø—É—Å–∫)
```bash
# –°—Ç–≤–æ—Ä–∏ /etc/systemd/system/vllm.service
[Unit]
Description=vLLM API Server
After=network.target

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/your/project
ExecStart=/usr/local/bin/vllm serve google/gemma-2-4b-it --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
# –ê–∫—Ç–∏–≤–∞—Ü—ñ—è
sudo systemctl enable vllm
sudo systemctl start vllm
sudo systemctl status vllm
```

### Docker (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
```dockerfile
FROM vllm/vllm-openai:latest

ENV MODEL_NAME=google/gemma-2-4b-it
ENV PORT=8000

CMD ["vllm", "serve", "${MODEL_NAME}", "--host", "0.0.0.0", "--port", "${PORT}"]
```

---

## üìä –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ç–∞ –º–µ—Ç—Ä–∏–∫–∏

### Prometheus –º–µ—Ç—Ä–∏–∫–∏
```bash
curl http://localhost:8000/metrics
```

### –õ–æ–≥–∏
```bash
# –î–∏–≤–∏—Ç–∏—Å—å –ª–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
journalctl -u vllm -f

# –ê–±–æ —è–∫—â–æ –∑–∞–ø—É—â–µ–Ω–æ –≤—Ä—É—á–Ω—É
tail -f vllm.log
```

---

## üêõ Troubleshooting

### –°–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è
```bash
# –ü–µ—Ä–µ–≤—ñ—Ä –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å GPU
nvidia-smi

# –ü–µ—Ä–µ–≤—ñ—Ä —â–æ –ø–æ—Ä—Ç –≤—ñ–ª—å–Ω–∏–π
netstat -tulpn | grep 8000

# –ó–º–µ–Ω—à –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
vllm serve google/gemma-2-4b-it --gpu-memory-utilization 0.7
```

### Out of Memory
```bash
# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é
vllm serve google/gemma-2-4b-it --quantization awq

# –ê–±–æ –∑–º–µ–Ω—à max-model-len
vllm serve google/gemma-2-4b-it --max-model-len 4096
```

### –ü–æ–≤—ñ–ª—å–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è
```bash
# –£–≤—ñ–º–∫–Ω–∏ prefix caching
vllm serve google/gemma-2-4b-it --enable-prefix-caching

# –ê–±–æ –∑–±—ñ–ª—å—à batch size
vllm serve google/gemma-2-4b-it --max-num-seqs 256
```

---

## üîê –ë–µ–∑–ø–µ–∫–∞

### –î–æ–¥–∞—Ç–∏ API key
```python
# –ü—Ä–∏ –∑–∞–ø—É—Å–∫—É —Å–µ—Ä–≤–µ—Ä–∞
vllm serve model --api-key "your-secret-key"

# –£ –∫–ª—ñ—î–Ω—Ç—ñ
client = VLLMSummarizerClient(
    api_key="your-secret-key"
)
```

### –û–±–º–µ–∂–µ–Ω–Ω—è –¥–æ—Å—Ç—É–ø—É
```bash
# –¢—ñ–ª—å–∫–∏ –ª–æ–∫–∞–ª—å–Ω–∏–π –¥–æ—Å—Ç—É–ø
vllm serve model --host 127.0.0.1

# –ó nginx reverse proxy
# –î–æ–¥–∞–π rate limiting, SSL, —Ç–æ—â–æ
```

---

## üìö –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è

- [vLLM Documentation](https://docs.vllm.ai/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Gemma Models](https://huggingface.co/google)

---

## üí° Tips

‚úÖ **–î–ª—è —Ä–æ–∑—Ä–æ–±–∫–∏:** –∑–∞–ø—É—Å–∫–∞–π —Å–µ—Ä–≤–µ—Ä –≤ –æ–∫—Ä–µ–º–æ–º—É —Ç–µ—Ä–º—ñ–Ω–∞–ª—ñ  
‚úÖ **–î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É:** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π systemd –∞–±–æ Docker  
‚úÖ **–î–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ:** —É–≤—ñ–º–∫–Ω–∏ prefix caching —Ç–∞ chunked prefill  
‚úÖ **–î–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ:** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é (AWQ, GPTQ)  
‚úÖ **–î–ª—è –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è:** –¥–æ–¥–∞–π tensor parallelism (`--tensor-parallel-size 2`)
