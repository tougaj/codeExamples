#!/usr/bin/env bash
# üöÄ –°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫—É vLLM OpenAI-compatible API —Å–µ—Ä–≤–µ—Ä–∞ –∑ Gemma 3

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
MODEL_NAME="google/gemma-3-4b-it"
PORT=8000
HOST="0.0.0.0"  # 0.0.0.0 –¥–ª—è –¥–æ—Å—Ç—É–ø—É –∑–∑–æ–≤–Ω—ñ, 127.0.0.1 —Ç—ñ–ª—å–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ
MAX_MODEL_LEN=8192
GPU_MEMORY_UTILIZATION=0.9

# üéØ –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞
echo "üöÄ –ó–∞–ø—É—Å–∫ vLLM OpenAI-compatible API —Å–µ—Ä–≤–µ—Ä–∞..."
echo "üì¶ –ú–æ–¥–µ–ª—å: $MODEL_NAME"
echo "üåê –ê–¥—Ä–µ—Å–∞: http://$HOST:$PORT"
echo ""

uv run vllm serve $MODEL_NAME \
    --host $HOST \
    --port $PORT \
    --max-model-len $MAX_MODEL_LEN \
    --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \
    --tensor-parallel-size 1 \
    --swap-space 8 \
    --enable-chunked-prefill \
    --dtype auto \
    --enable-prefix-caching \
    --disable-log-requests

    # --trust-remote-code \

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ—Ä–∏—Å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–∑–∞–∫–æ–º–µ–Ω—Ç–æ–≤–∞–Ω—ñ):
# --api-key YOUR_SECRET_KEY \           # –¥–æ–¥–∞—Ç–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—é
# --served-model-name gemma-3-4b \      # –∫–∞—Å—Ç–æ–º–Ω–∞ –Ω–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ –≤ API
# --max-num-seqs 256 \                  # –º–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö sequences
# --enable-chunked-prefill \            # –¥–ª—è –¥–æ–≤–≥–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤
# --quantization awq \                  # —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—à –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å

echo ""
echo "‚úÖ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω–æ!"
echo "üìö API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è: http://$HOST:$PORT/docs"
echo "‚ù§Ô∏è  Health check: http://$HOST:$PORT/health"
echo ""
echo "üõë –î–ª—è –∑—É–ø–∏–Ω–∫–∏ –Ω–∞—Ç–∏—Å–Ω–∏ Ctrl+C"
