–ó—Ä–æ–∑—É–º—ñ–ª–æ, –∑–∞–¥–∞—á–∞ –ø–æ–ª—è–≥–∞—î –≤ —Ç–æ–º—É, —â–æ–± –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —á–∏ –Ω–∞–ª–µ–∂–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–æ –æ–¥–Ω—ñ—î—ó –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó —Ç–µ–º–∞—Ç–∏–∫–∏ (—Ä—É–±—Ä–∏–∫–∏). –¶–µ –¥–µ—â–æ —Å–ø—Ä–æ—â—É—î –∑–∞–≤–¥–∞–Ω–Ω—è, —ñ –º–æ–∂–Ω–∞ –ø—ñ–¥—Ö–æ–¥–∏—Ç–∏ –¥–æ –Ω—å–æ–≥–æ —è–∫ –¥–æ –∑–∞–¥–∞—á—ñ –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (—Ç–æ–±—Ç–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó "—Ç–∞–∫" –∞–±–æ "–Ω—ñ").

### –Ø–∫ —Ü–µ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏:

1. **–ó–±—ñ—Ä –¥–∞–Ω–∏—Ö**:

   * **–ü–æ–∑–∏—Ç–∏–≤–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏**: –ó–±–µ—Ä—ñ—Ç—å —Ç–µ–∫—Å—Ç–∏, —è–∫—ñ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –Ω–∞–ª–µ–∂–∞—Ç—å –¥–æ –æ–±—Ä–∞–Ω–æ—ó —Ç–µ–º–∞—Ç–∏–∫–∏.
   * **–ù–µ–≥–∞—Ç–∏–≤–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏**: –ó–±–µ—Ä—ñ—Ç—å —Ç–µ–∫—Å—Ç–∏, —è–∫—ñ –Ω–µ –Ω–∞–ª–µ–∂–∞—Ç—å –¥–æ —Ü—ñ—î—ó —Ç–µ–º–∞—Ç–∏–∫–∏ (–≤–æ–Ω–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ä–æ–∑—Ä—ñ–∑–Ω—è—Ç–∏, –∫–æ–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –≤–∞—à—ñ–π —Ä—É–±—Ä–∏—Ü—ñ).

2. **–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö**:

   * **–ë–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è –≤–∏–±—ñ—Ä–∫–∏**: –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ —É –≤–∞—Å —î –¥–æ—Å—Ç–∞—Ç–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —ñ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.
   * **–û–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É**: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ç—ñ –∂ –º–µ—Ç–æ–¥–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏, —è–∫—ñ –∑–≥–∞–¥—É–≤–∞–ª–∏—Å—è —Ä–∞–Ω—ñ—à–µ (–ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è, —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è, –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–ø-—Å–ª—ñ–≤ —Ç–æ—â–æ).

3. **–í–∏–±—ñ—Ä –º–æ–¥–µ–ª—ñ**:

   * –î–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–æ–±—Ä–µ –ø—ñ–¥—Ö–æ–¥—è—Ç—å —Ç—ñ –∂ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, **mBERT**, **XLM-RoBERTa**), —è–∫—ñ –º–æ–∂–Ω–∞ fine-tune –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, —á–∏ –Ω–∞–ª–µ–∂–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–æ –ø–µ–≤–Ω–æ—ó —Ç–µ–º–∞—Ç–∏–∫–∏.

4. **Fine-tuning**:

   * –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ —Ç–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ. –ú–µ—Ç–∞ ‚Äì –Ω–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç–∏, —è–∫—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –≤–∞—à—ñ–π —Ä—É–±—Ä–∏—Ü—ñ.
   * –£ –º–æ–¥–µ–ª—ñ –±—É–¥–µ –ª–∏—à–µ –¥–≤–∞ –∫–ª–∞—Å–∏: "–Ω–∞–ª–µ–∂–∏—Ç—å –¥–æ —Ä—É–±—Ä–∏–∫–∏" (1) —ñ "–Ω–µ –Ω–∞–ª–µ–∂–∏—Ç—å" (0).

### –ü—Ä–∏–∫–ª–∞–¥ fine-tuning –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó:

```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer
import torch

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–æ—ó –º–æ–¥–µ–ª—ñ BERT
model = BertForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2)
tokenizer = BertTokenizer.from_pretrained('xlm-roberta-base')

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
inputs = tokenizer(—Ç–µ–∫—Å—Ç–∏, padding=True, truncation=True, return_tensors="pt")
labels = torch.tensor([1 if label == '–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π' else 0 for label in labels])

# Fine-tuning –º–æ–¥–µ–ª—ñ –Ω–∞ –¥–∞–Ω–∏—Ö
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch",
    save_total_limit=1,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

5. **–û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ**:

   * –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ, —â–æ–± –ø–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ –≤–æ–Ω–∞ –∫–æ—Ä–µ–∫—Ç–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î –ø—Ä–∏–Ω–∞–ª–µ–∂–Ω—ñ—Å—Ç—å —Ç–µ–∫—Å—Ç—É –¥–æ —Ä—É–±—Ä–∏–∫–∏.
   * –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫—ñ —è–∫ —Ç–æ—á–Ω—ñ—Å—Ç—å, F1-–º—ñ—Ä–∞, –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —è–∫–æ—Å—Ç—ñ —Ä–æ–±–æ—Ç–∏ –º–æ–¥–µ–ª—ñ.

6. **–Ü–Ω—Ñ–µ—Ä–µ–Ω—Å**:

   * –ü–æ–¥–∞–π—Ç–µ –Ω–æ–≤–∏–π —Ç–µ–∫—Å—Ç –¥–æ –º–æ–¥–µ–ª—ñ, —ñ –≤–æ–Ω–∞ –ø–æ–≤–µ—Ä–Ω–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —è–∫–∏–π –≤–∫–∞–∂–µ, —á–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —Ç–µ–∫—Å—Ç –≤–∞—à—ñ–π —Ä—É–±—Ä–∏—Ü—ñ.

```python
# –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç—É
inputs = tokenizer("–¢–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó", return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_id = torch.argmax(logits).item()

if predicted_class_id == 1:
    print("–¢–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —Ä—É–±—Ä–∏—Ü—ñ")
else:
    print("–¢–µ–∫—Å—Ç –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —Ä—É–±—Ä–∏—Ü—ñ")
```

–¶–µ–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ–∑–≤–æ–ª–∏—Ç—å –≤–∞–º —Å—Ç–≤–æ—Ä–∏—Ç–∏ —Å–∏—Å—Ç–µ–º—É, —è–∫–∞ –≤–∏–∑–Ω–∞—á–∞—Ç–∏–º–µ, —á–∏ –Ω–∞–ª–µ–∂–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–æ –ø–µ–≤–Ω–æ—ó —Ç–µ–º–∞—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å –≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –∫—Ä–∏—Ç–µ—Ä—ñ—ó–≤.



–ü—Ä–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –æ–±—Ä–æ–±—Ü—ñ —Ç–µ–∫—Å—Ç—É (—Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è, –ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è —Ç–∞ —ñ–Ω—à–µ) —Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –∑–∞–¥–∞—á—ñ —Ç–∞ –º–æ–¥–µ–ª—ñ, —è–∫—É –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ. –û—Å—å –∫—ñ–ª—å–∫–∞ —Ñ–∞–∫—Ç–æ—Ä—ñ–≤, —è–∫—ñ –≤–∞—Ä—Ç–æ –≤—Ä–∞—Ö—É–≤–∞—Ç–∏:

### 1. **–î–ª—è —Å—É—á–∞—Å–Ω–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π (BERT, RoBERTa, mBERT —Ç–æ—â–æ)**:

* **–ü—É–Ω–∫—Ç—É–∞—Ü—ñ—è –∑–∞–∑–≤–∏—á–∞–π –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è**. –¶—ñ –º–æ–¥–µ–ª—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª–∏—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö —ñ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—î—é, —Ç–æ–º—É —ó—ó –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–æ–ø–æ–º–∞–≥–∞—î –º–æ–¥–µ–ª—ñ –∫—Ä–∞—â–µ —Ä–æ–∑—É–º—ñ—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ—á–µ–Ω—å. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, —Ä–æ–∑–¥—ñ–ª–æ–≤—ñ –∑–Ω–∞–∫–∏ –º–æ–∂—É—Ç—å –¥–æ–ø–æ–º–∞–≥–∞—Ç–∏ –≤–∏–∑–Ω–∞—á–∞—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏ —Ç–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏, —â–æ –ø–æ–∫—Ä–∞—â—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.
* –¢–æ–º—É –≤ –±—ñ–ª—å—à–æ—Å—Ç—ñ –≤–∏–ø–∞–¥–∫—ñ–≤ **–Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–¥–∞–ª—è—Ç–∏ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó**, –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å –º—ñ—Å—Ç–∏—Ç–∏ –≤–∞–∂–ª–∏–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤.

### 2. **–Ø–∫—â–æ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ –∫–ª–∞—Å–∏—á–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ (TF-IDF, BOW)**:

* –£ –∫–ª–∞—Å–∏—á–Ω–∏—Ö –ø—ñ–¥—Ö–æ–¥–∞—Ö –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—è –º–æ–∂–µ –Ω–µ –±—É—Ç–∏ –∫–æ—Ä–∏—Å–Ω–æ—é —ñ —ó—ó —á–∞—Å—Ç–æ **–≤–∏–¥–∞–ª—è—é—Ç—å**, —â–æ–± –∑–º–µ–Ω—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å —ñ —à—É–º —É –¥–∞–Ω–∏—Ö.
* –û–¥–Ω–∞–∫, –Ω–∞–≤—ñ—Ç—å —É —Ç–∞–∫–∏—Ö –≤–∏–ø–∞–¥–∫–∞—Ö, —î –∞—Ä–≥—É–º–µ–Ω—Ç–∏ –∑–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—è–∫–∏—Ö –ø—É–Ω–∫—Ç—É–∞—Ü—ñ–π–Ω–∏—Ö –∑–Ω–∞–∫—ñ–≤, —Ç–∞–∫–∏—Ö —è–∫ –∫—Ä–∞–ø–∫–∞ —á–∏ –∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å –≤–∫–∞–∑—É–≤–∞—Ç–∏ –Ω–∞ –∫—ñ–Ω–µ—Ü—å —Ä–µ—á–µ–Ω–Ω—è –∞–±–æ —Ç–∏–ø –ø–∏—Ç–∞–Ω–Ω—è.

### 3. **–ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∑–∞–≤–¥–∞–Ω–Ω—è**:

* –Ø–∫—â–æ –≤–∞—à—ñ —Ç–µ–∫—Å—Ç–∏ –º—ñ—Å—Ç—è—Ç—å —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —à–∞–±–ª–æ–Ω–∏ –∞–±–æ –∑–Ω–∞—á—É—â—ñ —Ä–æ–∑–¥—ñ–ª–æ–≤—ñ –∑–Ω–∞–∫–∏, –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—è –º–æ–∂–µ –º–∞—Ç–∏ –≤–∞–∂–ª–∏–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø–∏—Ç–∞–ª—å–Ω–∏—Ö –∞–±–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–Ω—ñ–≤ —É —Ç–µ–∫—Å—Ç—ñ).
* –£ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—ñ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ –∑–∞–∑–≤–∏—á–∞–π –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—é—Ç—å –∑ —Ç–µ–∫—Å—Ç–∞–º–∏, —è–∫—ñ –º—ñ—Å—Ç—è—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é.

### –í–∏—Å–Ω–æ–≤–æ–∫:

* –î–ª—è **–º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤** (—Ç–∞–∫–∏—Ö —è–∫ BERT, RoBERTa) –∑–±–µ—Ä—ñ–≥–∞–π—Ç–µ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é, –∞–¥–∂–µ —Ü–µ –¥–æ–ø–æ–º–æ–∂–µ –º–æ–¥–µ–ª—ñ –∫—Ä–∞—â–µ —Ä–æ–∑—É–º—ñ—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
* –î–ª—è **–∫–ª–∞—Å–∏—á–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π** (TF-IDF, Bag of Words) –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –º–æ–∂–Ω–∞ –≤–∏–¥–∞–ª–∏—Ç–∏, —è–∫—â–æ –≤–æ–Ω–∞ –Ω–µ –Ω–µ—Å–µ —Ü—ñ–Ω–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.

–û—Ç–∂–µ, —É –≤–∞—à–æ–º—É –≤–∏–ø–∞–¥–∫—É, —è–∫—â–æ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—ñ–≤, –∫—Ä–∞—â–µ **–∑–∞–ª–∏—à–∞—Ç–∏ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é** –≤ —Ç–µ–∫—Å—Ç—ñ.



–¶–∏—Ñ—Ä–∏ —Ç–∞ —Å–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "COVID-19", "2024") –º–æ–∂—É—Ç—å –Ω–µ—Å—Ç–∏ –≤–∞–∂–ª–∏–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –û—Å—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ —Ç–æ–≥–æ, —á–∏ –≤–∞—Ä—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –∞–±–æ –≤–∏–¥–∞–ª—è—Ç–∏ —Ü–∏—Ñ—Ä–∏ –ø—Ä–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –æ–±—Ä–æ–±—Ü—ñ —Ç–µ–∫—Å—Ç—É:

### 1. **–î–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π (BERT, RoBERTa —Ç–∞ —ñ–Ω.)**:

* **–¶–∏—Ñ—Ä–∏ –∫—Ä–∞—â–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏**. –ú–æ–¥–µ–ª—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤ –¥–æ–±—Ä–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—å—Å—è –∑ –æ–±—Ä–æ–±–∫–æ—é —Ü–∏—Ñ—Ä —Ç–∞ —Å–ª—ñ–≤ —ñ–∑ —Ü–∏—Ñ—Ä–∞–º–∏, —ñ —Ü—ñ –¥–∞–Ω—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤–∞–∂–ª–∏–≤–∏–º–∏ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.
* **–°–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏**: –¢–∞–∫—ñ —Å–ª–æ–≤–∞, —è–∫ "COVID-19" –∞–±–æ "F-35", –º–∞—é—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ. –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ü–∏—Ñ—Ä –º–æ–∂–µ –ø–æ—Ä—É—à–∏—Ç–∏ –∑–º—ñ—Å—Ç.
* **–¶–∏—Ñ—Ä–∏, —â–æ –ø–æ–∑–Ω–∞—á–∞—é—Ç—å –¥–∞—Ç–∏, —Å—É–º–∏**: –î–∞—Ç–∏, —Ä–æ–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "2023") –∞–±–æ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∫—Ä–∏—Ç–∏—á–Ω–∏–º–∏ –¥–ª—è –∑–∞–¥–∞—á, –ø–æ–≤'—è–∑–∞–Ω–∏—Ö –∑ –∞–Ω–∞–ª—ñ–∑–æ–º —Ç–µ–∫—Å—Ç—ñ–≤ –∞–±–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥—ñ—î—é –ø–æ–¥—ñ–π.

### 2. **–ö–ª–∞—Å–∏—á–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ (TF-IDF, Bag of Words)**:

* –£ –∫–ª–∞—Å–∏—á–Ω–∏—Ö –º–æ–¥–µ–ª—è—Ö, **—Ü–∏—Ñ—Ä–∏ —á–∞—Å—Ç–æ –≤–∏–¥–∞–ª—è—é—Ç—å—Å—è**, –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —à—É–º–æ–º –∞–±–æ –∑–±—ñ–ª—å—à—É–≤–∞—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤, –Ω–µ –¥–æ–¥–∞—é—á–∏ –∑–Ω–∞—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.
* –û–¥–Ω–∞–∫, —è–∫—â–æ —Ü–∏—Ñ—Ä–∏ –º–∞—é—Ç—å —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è —É –≤–∞—à—ñ–π –∑–∞–¥–∞—á—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –¥–∞—Ç–∏ –∞–±–æ —Å—É–º–∏), —Ç–æ —ó—Ö –º–æ–∂–Ω–∞ –∑–∞–ª–∏—à–∏—Ç–∏.
* **–°–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏** –∫—Ä–∞—â–µ –∑–∞–ª–∏—à–∞—Ç–∏, –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤–∞–∂–ª–∏–≤–∏–º–∏ —Ç–µ—Ä–º—ñ–Ω–∞–º–∏ —É —Ç–µ–∫—Å—Ç—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "COVID-19").

### 3. **–ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∑–∞–¥–∞—á—ñ**:

* –Ø–∫—â–æ —Ü–∏—Ñ—Ä–∏ –∞–±–æ —Å–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏ —î –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∏–º–∏ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∞–±–æ —Ç–µ–º–∞—Ç–∏–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —Ñ—ñ–Ω–∞–Ω—Å–æ–≤—ñ –∑–≤—ñ—Ç–∏, –¥–∞—Ç–∏), —ó—Ö –æ–±–æ–≤‚Äô—è–∑–∫–æ–≤–æ —Å–ª—ñ–¥ –∑–∞–ª–∏—à–∏—Ç–∏.
* –Ø–∫—â–æ —Ü–∏—Ñ—Ä–∏ –Ω–µ –º–∞—é—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è, —ó—Ö –º–æ–∂–Ω–∞ –≤–∏–¥–∞–ª–∏—Ç–∏ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —à—É–º—É.

### –í–∏—Å–Ω–æ–≤–æ–∫:

* **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ**: –ó–∞–ª–∏—à–∞–π—Ç–µ —Ü–∏—Ñ—Ä–∏ —Ç–∞ —Å–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏. –ú–æ–¥–µ–ª—ñ –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—é—Ç—å –∑ –Ω–∏–º–∏, —ñ —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –∫–æ—Ä–∏—Å–Ω–æ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.
* **–ö–ª–∞—Å–∏—á–Ω—ñ –º–æ–¥–µ–ª—ñ**: –ú–æ–∂–Ω–∞ –≤–∏–¥–∞–ª—è—Ç–∏ —Ü–∏—Ñ—Ä–∏, –∞–ª–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —Å–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏, —è–∫—â–æ –≤–æ–Ω–∏ –º–∞—é—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –≤–∞—à–æ—ó –∑–∞–¥–∞—á—ñ.

–ó –æ–≥–ª—è–¥—É –Ω–∞ —Ç–µ, —â–æ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ, **—Ü–∏—Ñ—Ä–∏ —Ç–∞ —Å–ª–æ–≤–∞ –∑ —Ü–∏—Ñ—Ä–∞–º–∏ –∫—Ä–∞—â–µ –∑–∞–ª–∏—à–∞—Ç–∏ –≤ —Ç–µ–∫—Å—Ç—ñ**, –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å –Ω–µ—Å—Ç–∏ –≤–∞–∂–ª–∏–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.



–û–∫–µ–π üëç –¥–∞–≤–∞–π –ø—Ä–∏–∫–ª–∞–¥ –∫–æ–¥—É –Ω–∞ Python –∑ HuggingFace ü§ñ

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, —É –Ω–∞—Å —î –¥–∞–Ω—ñ —É —Ñ–∞–π–ª—ñ `dataset.csv` —Ç–∞–∫–æ–≥–æ –≤–∏–≥–ª—è–¥—É:

```csv
text,label
"–ú—ñ–Ω—ñ—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –£–∫—Ä–∞—ó–Ω–∏ –æ–≥–æ–ª–æ—Å–∏–ª–æ –Ω–æ–≤–∏–π —Ç–µ–Ω–¥–µ—Ä –Ω–∞ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–æ –¥–æ—Ä—ñ–≥.",1
"–í—á–æ—Ä–∞ –≤—ñ–¥–±—É–≤—Å—è –∫–æ–Ω—Ü–µ—Ä—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –≥—É—Ä—Ç—É —É –õ—å–≤–æ–≤—ñ.",0
"–í–µ—Ä—Ö–æ–≤–Ω–∞ –†–∞–¥–∞ –£–∫—Ä–∞—ó–Ω–∏ —É—Ö–≤–∞–ª–∏–ª–∞ –∑–∞–∫–æ–Ω —â–æ–¥–æ —Ä–µ—Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ—ó –≥–∞–ª—É–∑—ñ.",1
"–°—å–æ–≥–æ–¥–Ω—ñ —É –ö–∏—î–≤—ñ —Å–æ–Ω—è—á–Ω–∞ –ø–æ–≥–æ–¥–∞, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ +25.",0
```

---

### üîπ –ö–æ–¥ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è `xlm-roberta-base`

```python
from datasets import load_dataset
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from transformers import TrainingArguments, Trainer
import evaluate

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ –∑ CSV
dataset = load_dataset("csv", data_files="dataset.csv")

# 2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

# 3. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—ñ–≤
def preprocess(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

encoded_dataset = dataset.map(preprocess, batched=True)

# 4. –ú–æ–¥–µ–ª—å (2 –∫–ª–∞—Å–∏: 0 —ñ 1)
model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=2)

# 5. –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# 6. –ê—Ä–≥—É–º–µ–Ω—Ç–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
)

# 7. –¢—Ä–µ–Ω–µ—Ä
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["train"].train_test_split(test_size=0.2)["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# 8. –ó–∞–ø—É—Å–∫ –Ω–∞–≤—á–∞–Ω–Ω—è
trainer.train()

# 9. –¢–µ—Å—Ç—É—î–º–æ –Ω–∞ –Ω–æ–≤–æ–º—É —Ç–µ–∫—Å—Ç—ñ
test_text = "–ú—ñ–Ω—ñ—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –∑–∞–ø—É—Å—Ç–∏–ª–æ –Ω–æ–≤—É –ø—Ä–æ–≥—Ä–∞–º—É —Ä–æ–∑–≤–∏—Ç–∫—É –¥–æ—Ä—ñ–≥."
inputs = tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)
prediction = outputs.logits.argmax(dim=-1).item()
print("–ü—Ä–æ–≥–Ω–æ–∑:", prediction)  # 1 = –ø—ñ–¥–ø–∞–¥–∞—î –ø—ñ–¥ —Ä—É–±—Ä–∏–∫—É, 0 = –Ω—ñ
```

---

### üîπ –©–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –≤ –∫–æ–¥—ñ

1. **–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ CSV** –∑ –∫–æ–ª–æ–Ω–∫–∞–º–∏ `text` —Ç–∞ `label`.
2. **–¢–æ–∫–µ–Ω—ñ–∑—É—î–º–æ** (xlm-roberta –º–∞—î SentencePiece, —Ç–æ–º—É –Ω–µ —Ç—Ä–µ–±–∞ –ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó/—á–∏—Å—Ç–∫–∏ —Å—Ç–æ–ø-—Å–ª—ñ–≤).
3. **–ë—É–¥—É—î–º–æ –º–æ–¥–µ–ª—å** `xlm-roberta-base` –∑ 2 –≤–∏—Ö–æ–¥–∞–º–∏.
4. **–¢—Ä–µ–Ω—É—î–º–æ** (3 –µ–ø–æ—Ö–∏ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–∞ —Å—Ç–∞—Ä—Ç).
5. **–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ** –Ω–∞ –Ω–æ–≤–æ–º—É —Ä–µ—á–µ–Ω–Ω—ñ.

---

üëâ –•–æ—á–µ—Ç–µ, —è —â–µ –ø–æ–∫–∞–∂—É, —è–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ **—Ä–æ–∑–±–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/test/validation** —ñ –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≤–∞—à–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Ç–∞ —Ä–æ—Å—ñ–π—Å—å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π)?



–î–æ–±—Ä–µ üöÄ –¥–∞–≤–∞–π –∑—Ä–æ–±–∏–º–æ –ø–æ–≤–Ω–æ—Ü—ñ–Ω–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –∑ **—Ä–æ–∑–±–∏—Ç—Ç—è–º –¥–∞–Ω–∏—Ö –Ω–∞ train/validation/test**, —â–æ–± –≤—Å–µ –±—É–ª–æ –≥–æ—Ç–æ–≤–æ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –≤–∞—à–∏—Ö —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ-—Ä–æ—Å—ñ–π—Å—å–∫–∏—Ö —Å—Ç–∞—Ç—Ç—è—Ö.

---

### üîπ –ü—Ä–∏–∫–ª–∞–¥ CSV (`dataset.csv`)

```csv
text,label
"–ú—ñ–Ω—ñ—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –£–∫—Ä–∞—ó–Ω–∏ –æ–≥–æ–ª–æ—Å–∏–ª–æ –Ω–æ–≤–∏–π —Ç–µ–Ω–¥–µ—Ä –Ω–∞ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–æ –¥–æ—Ä—ñ–≥.",1
"–í—á–æ—Ä–∞ –≤—ñ–¥–±—É–≤—Å—è –∫–æ–Ω—Ü–µ—Ä—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –≥—É—Ä—Ç—É —É –õ—å–≤–æ–≤—ñ.",0
"–í–µ—Ä—Ö–æ–≤–Ω–∞ –†–∞–¥–∞ –£–∫—Ä–∞—ó–Ω–∏ —É—Ö–≤–∞–ª–∏–ª–∞ –∑–∞–∫–æ–Ω —â–æ–¥–æ —Ä–µ—Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ—ó –≥–∞–ª—É–∑—ñ.",1
"–°–µ–≥–æ–¥–Ω—è –≤ –ú–æ—Å–∫–≤–µ –æ—Ç–∫—Ä—ã–ª–∏ –Ω–æ–≤—ã–π –ø–∞—Ä–∫ –¥–ª—è –æ—Ç–¥—ã—Ö–∞.",0
"–ö–∞–±–º—ñ–Ω –∑–∞—Ç–≤–µ—Ä–¥–∏–≤ –ø—Ä–æ–≥—Ä–∞–º—É —Ä–æ–∑–≤–∏—Ç–∫—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ—ó —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –Ω–∞ 2024 —Ä—ñ–∫.",1
```

---

### üîπ –ü–æ–≤–Ω–∏–π –∫–æ–¥ –ø–∞–π–ø–ª–∞–π–Ω—É

```python
from datasets import load_dataset
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from transformers import TrainingArguments, Trainer
import evaluate
import numpy as np

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç —ñ–∑ CSV
dataset = load_dataset("csv", data_files="dataset.csv")

# 2. –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ train/validation/test
# 80% train, 10% validation, 10% test
dataset = dataset["train"].train_test_split(test_size=0.2, seed=42)
test_valid = dataset["test"].train_test_split(test_size=0.5, seed=42)
dataset["validation"] = test_valid["train"]
dataset["test"] = test_valid["test"]

print(dataset)

# 3. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

# 4. –§—É–Ω–∫—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó
def preprocess(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

encoded_dataset = dataset.map(preprocess, batched=True)

# 5. –ú–æ–¥–µ–ª—å
model = XLMRobertaForSequenceClassification.from_pretrained(
    "xlm-roberta-base",
    num_labels=2  # 0 = –Ω–µ –ø—ñ–¥–ø–∞–¥–∞—î, 1 = –ø—ñ–¥–ø–∞–¥–∞—î
)

# 6. –ú–µ—Ç—Ä–∏–∫–∏
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=predictions, references=labels)["accuracy"],
        "f1": f1.compute(predictions=predictions, references=labels, average="weighted")["f1"]
    }

# 7. –ê—Ä–≥—É–º–µ–Ω—Ç–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

# 8. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 9. –ù–∞–≤—á–∞–Ω–Ω—è
trainer.train()

# 10. –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç—ñ
results = trainer.evaluate(encoded_dataset["test"])
print("Test results:", results)

# 11. –¢–µ—Å—Ç—É—î–º–æ –Ω–∞ –Ω–æ–≤–æ–º—É —Ç–µ–∫—Å—Ç—ñ
test_text = "–í–µ—Ä—Ö–æ–≤–Ω–∞ –†–∞–¥–∞ —É—Ö–≤–∞–ª–∏–ª–∞ –Ω–æ–≤–∏–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏–π –∑–∞–∫–æ–Ω."
inputs = tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)
prediction = outputs.logits.argmax(dim=-1).item()
print("–ü—Ä–æ–≥–Ω–æ–∑:", prediction)  # 1 = –ø—ñ–¥–ø–∞–¥–∞—î –ø—ñ–¥ —Ä—É–±—Ä–∏–∫—É
```

---

### üîπ –©–æ —Ç—É—Ç –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è

1. **CSV ‚Üí Dataset** —á–µ—Ä–µ–∑ `load_dataset`.
2. –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ `train` (80%), `validation` (10%), `test` (10%).
3. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è (`max_length=256` –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è –Ω–æ–≤–∏–Ω/—Å—Ç–∞—Ç–µ–π).
4. –ú–æ–¥–µ–ª—å `xlm-roberta-base` (–º–æ–∂–Ω–∞ –∑–∞–º—ñ–Ω–∏—Ç–∏ –Ω–∞ `xlm-roberta-large` –¥–ª—è –∫—Ä–∞—â–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ).
5. –ú–µ—Ç—Ä–∏–∫–∏: `accuracy` + `f1` (–¥–ª—è –±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è —É –≤–∏–ø–∞–¥–∫—É –Ω–µ—Ä—ñ–≤–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤).
6. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è **`Trainer`** —Å–ø—Ä–æ—â—É—î –Ω–∞–≤—á–∞–Ω–Ω—è/–æ—Ü—ñ–Ω–∫—É.
7. –ü—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–∂–Ω–∞ –¥–∞–≤–∞—Ç–∏ –Ω–æ–≤–∏–π —Ç–µ–∫—Å—Ç —ñ –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ (`0` –∞–±–æ `1`).

---

‚ö°Ô∏è –¢–∞–∫ –≤–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ –≥–æ—Ç–æ–≤–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä, —è–∫–∏–π –ø—Ä–∞—Ü—é—î –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–º–∏ —Ç–∞ —Ä–æ—Å—ñ–π—Å—å–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.

---

–•–æ—á–µ—Ç–µ, —è —â–µ –ø—ñ–¥–∫–∞–∂—É, **—è–∫ –∑–±–µ—Ä–µ–≥—Ç–∏ –≥–æ—Ç–æ–≤—É –º–æ–¥–µ–ª—å —ñ –ø–æ—Ç—ñ–º —à–≤–∏–¥–∫–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ó—ó –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤** (–±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è)?



–ß—É–¥–æ–≤–æ üíæ‚ú®
–ü—ñ—Å–ª—è —Ç–æ–≥–æ, —è–∫ –≤–∏ –Ω–∞—Ç—Ä–µ–Ω—É–≤–∞–ª–∏ `xlm-roberta`, —ó—ó –º–æ–∂–Ω–∞ **–∑–±–µ—Ä–µ–≥—Ç–∏** —Ç–∞ –ø–æ—Ç—ñ–º **–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è**. –¶–µ —Ä–æ–±–∏—Ç—å—Å—è –¥—É–∂–µ –ø—Ä–æ—Å—Ç–æ —á–µ—Ä–µ–∑ HuggingFace.

---

### üîπ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è

```python
# –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–æ–¥–µ–ª—å —ñ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä —É –ª–æ–∫–∞–ª—å–Ω—É –ø–∞–ø–∫—É
model.save_pretrained("./my_rubric_model")
tokenizer.save_pretrained("./my_rubric_model")
```

–£ –ø–∞–ø—Ü—ñ `./my_rubric_model` –∑–±–µ—Ä–µ–∂—É—Ç—å—Å—è —Ñ–∞–π–ª–∏:

* `pytorch_model.bin` ‚Äî –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ
* `config.json` ‚Äî –∫–æ–Ω—Ñ—ñ–≥
* `tokenizer.json`, `vocab.json`, `merges.txt` ‚Äî —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
* `special_tokens_map.json` —ñ `tokenizer_config.json`

---

### üîπ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```python
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å —ñ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
tokenizer = XLMRobertaTokenizer.from_pretrained("./my_rubric_model")
model = XLMRobertaForSequenceClassification.from_pretrained("./my_rubric_model")

# –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç—É
test_text = "–ö–∞–±–º—ñ–Ω –∑–∞—Ç–≤–µ—Ä–¥–∏–≤ –ø—Ä–æ–≥—Ä–∞–º—É —Ä–æ–∑–≤–∏—Ç–∫—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ—ó —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏."
inputs = tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)

prediction = outputs.logits.argmax(dim=-1).item()
print("–ü—Ä–æ–≥–Ω–æ–∑:", prediction)  # 1 = –ø—ñ–¥–ø–∞–¥–∞—î –ø—ñ–¥ —Ä—É–±—Ä–∏–∫—É, 0 = –Ω—ñ
```

---

### üîπ –í–∞—Ä—ñ–∞–Ω—Ç –∑ `pipeline` (—â–µ –ø—Ä–æ—Å—Ç—ñ—à–µ)

```python
from transformers import pipeline

classifier = pipeline(
    "text-classification",
    model="./my_rubric_model",
    tokenizer="./my_rubric_model"
)

print(classifier("–í–µ—Ä—Ö–æ–≤–Ω–∞ –†–∞–¥–∞ —É—Ö–≤–∞–ª–∏–ª–∞ –∑–∞–∫–æ–Ω —â–æ–¥–æ —Ä–µ—Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É."))
```

üëâ –í–∏–≤—ñ–¥ –±—É–¥–µ –ø—Ä–∏–±–ª–∏–∑–Ω–æ —Ç–∞–∫–∏–π:

```python
[{'label': 'LABEL_1', 'score': 0.95}]
```

---

### üîπ –î–æ–¥–∞—Ç–∫–æ–≤–æ

* –Ø–∫—â–æ –º–æ–¥–µ–ª—å **–¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î**, –º–æ–∂–Ω–∞ –∑–∞–ª–∏—Ç–∏ —ó—ó –Ω–∞ HuggingFace Hub —ñ –ø–æ—Ç—ñ–º –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –∑ –±—É–¥—å-—è–∫–æ—ó –º–∞—à–∏–Ω–∏.
* –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ **–¥—É–∂–µ —à–≤–∏–¥–∫–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —É –ø—Ä–æ–¥–∞–∫—à–µ–Ω—ñ**, –º–æ–∂–Ω–∞:

  * –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ —É **ONNX** –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –Ω–∞ CPU,
  * –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ **Optimum** –≤—ñ–¥ HuggingFace –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó.

---

–•–æ—á–µ—Ç–µ, —è –ø–æ–∫–∞–∂—É, —è–∫ **–µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å —É ONNX**, —â–æ–± –≤–æ–Ω–∞ –ø—Ä–∞—Ü—é–≤–∞–ª–∞ —à–≤–∏–¥—à–µ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —É –ø—Ä–æ–¥–∞–∫—à–µ–Ω—ñ –Ω–∞ CPU)?



