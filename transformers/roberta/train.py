#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BysreyV-vu2qXh1SINBLBM4Wloaoj5k3
"""

# pip install "transformers[sentencepiece]" datasets evaluate torch scikit-learn
# pip install transformers[torch]

from datasets import load_dataset
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from transformers import TrainingArguments, Trainer
import evaluate

dataset = load_dataset("csv", data_files="./data/dataset.csv")

tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

def preprocess(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

encoded_dataset = dataset.map(preprocess, batched=True)

model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=2)

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# training_args = TrainingArguments(
#     output_dir="./results",
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=2e-5,
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     num_train_epochs=3,
#     weight_decay=0.01,
#     logging_dir="./logs",
#     logging_steps=50,
#     load_best_model_at_end=True,
# )

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,               # Коефіцієнт регуляризації
    logging_dir="./logs",
    eval_strategy="steps",   # ← Увага: eval_strategy, не evaluation_strategy
    eval_steps=10,
    learning_rate=2e-5,
    logging_steps=50,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["train"].train_test_split(test_size=0.2)["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

# Зберігаємо модель і токенайзер у локальну папку
model.save_pretrained("./my_rubric_model")
tokenizer.save_pretrained("./my_rubric_model")
