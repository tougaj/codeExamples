import re
from collections import Counter
from pprint import pprint

import hdbscan
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from data import messages


def remove_html_tags(text: str) -> str:
    """
    –í–∏–¥–∞–ª—è—î –≤—Å—ñ HTML-—Ç–µ–≥–∏ –∑ —Ç–µ–∫—Å—Ç—É, –∑–∞–ª–∏—à–∞—é—á–∏ –ª–∏—à–µ –≤–º—ñ—Å—Ç.

    Args:
        text (str): –í—Ö—ñ–¥–Ω–∏–π —Ä—è–¥–æ–∫, —â–æ –º–æ–∂–µ –º—ñ—Å—Ç–∏—Ç–∏ HTML-—Ç–µ–≥–∏.

    Returns:
        str: –¢–µ–∫—Å—Ç –±–µ–∑ HTML-—Ç–µ–≥—ñ–≤.
    """
    clean = re.sub(r'<[^>]+>', '', text)
    return clean


def cluster_title_centroid(texts, embeddings):
    centroid = embeddings.mean(axis=0, keepdims=True)
    sims = cosine_similarity(centroid, embeddings)[0]
    best_idx = sims.argmax()
    # return texts[best_idx][:200]  # –æ–±—Ä—ñ–∑–∞—î–º–æ
    return texts[best_idx]


def cluster_centroid_and_top_texts(texts, embeddings, top_k=10, preview_len=120):
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î:
    - centroid (np.array)
    - representative_text (str)
    - top_texts: —Å–ø–∏—Å–æ–∫ dict {text, similarity, index}
    """

    # 1Ô∏è‚É£ —Ü–µ–Ω—Ç—Ä–æ—ó–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ üß†
    centroid = embeddings.mean(axis=0, keepdims=True)

    # 2Ô∏è‚É£ cosine similarity –¥–æ –≤—Å—ñ—Ö —Ç–µ–∫—Å—Ç—ñ–≤
    sims = cosine_similarity(centroid, embeddings)[0]

    # 3Ô∏è‚É£ —ñ–Ω–¥–µ–∫—Å–∏ top-k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ (—Å–ø–∞–¥–∞–Ω–Ω—è)
    top_indices = np.argsort(sims)[::-1][:top_k]

    top_texts = [
        {
            "index": int(i),
            "similarity": float(sims[i]),
            "text": texts[i][:preview_len]
        }
        for i in top_indices
    ]

    representative_text = top_texts[0]["text"]

    return centroid[0], representative_text, top_texts


def main():

    texts = [remove_html_tags(text)[:1000] for text in messages]

    model = SentenceTransformer(
        # "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        # "sentence-transformers/all-MiniLM-L6-v2"
        # "google/embeddinggemma-300m"
        "Qwen/Qwen3-Embedding-0.6B"
        # "Qwen/Qwen3-Embedding-8B"
    )

    print("‚ÑπÔ∏è Calculating embeddings...")
    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True  # –í–ê–ñ–õ–ò–í–û –¥–ª—è HDBSCAN
    )

    # for e in embeddings:
    #     pprint(e)

    print("‚ÑπÔ∏è Clustering...")
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=7,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        min_samples=3,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É

        # –ü—Ä–∏ –Ω–∏–∑—å–∫—ñ–π –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
        # min_cluster_size=3,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        # min_samples=2,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É

        # –†–æ–±–æ—á–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
        # min_cluster_size=7,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        # min_samples=3,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É

        # –ó–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–æ GPT
        # min_cluster_size=5,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        # min_samples=3,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É

        metric="euclidean",      # –∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ = cosine
        cluster_selection_method="eom"
    )

    labels = clusterer.fit_predict(embeddings)

    # –≥—Ä—É–ø—É—î–º–æ —Ç–µ–∫—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö üì¶
    clusters = {}
    for text, label in zip(texts, labels):
        clusters.setdefault(label, []).append(text)

    # —Å–æ—Ä—Ç—É—î–º–æ –∫–ª–∞—Å—Ç–µ—Ä–∏ –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—é —Ç–µ–∫—Å—Ç—ñ–≤ (—Å–ø–∞–¥–∞–Ω–Ω—è ‚¨áÔ∏è)
    sorted_clusters = sorted(
        clusters.items(),
        key=lambda item: len(item[1]),
        reverse=True
    )

    # –≤–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç üñ®Ô∏è
    labels_count = len(sorted_clusters)
    for index, (label, items) in enumerate(sorted_clusters, 1):
        # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ (–Ω–∞–π–±–ª–∏–∂—á–∏–π —Ç–µ–∫—Å—Ç)
        texts_cluster = items
        embeds_cluster = embeddings[[i for i, l in enumerate(labels) if l == label]]

        centroid, title_text, top_texts = cluster_centroid_and_top_texts(texts_cluster, embeds_cluster, preview_len=250)
        # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ (–Ω–∞–π–±–ª–∏–∂—á–∏–π —Ç–µ–∫—Å—Ç)
        # texts_cluster = items
        # embeds_cluster = embeddings[[i for i, l in enumerate(labels) if l == label]]
        # title = cluster_title_centroid(texts_cluster, embeds_cluster)

        print(f"\nüì¶ CLUSTER {index} of {labels_count} (label: {label}) ({len(items)} messages)")
        print(f"üì∞ {title_text}")
        # pprint(top_texts)
        pprint([f"üìê {item["similarity"]*100:.1f} % {item["text"]}" for item in top_texts[:(10 if label != -1 else 20)]])
        # for item in items[:(10 if label != -1 else 20)]:
        #     print(f"üîµ {item[:200]}")

    pprint(Counter(labels))


if __name__ == "__main__":
    main()
