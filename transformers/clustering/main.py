from sentence_transformers import SentenceTransformer
from pprint import pprint
import hdbscan
from collections import Counter

def main():
    _texts = [
        "–ù–∞ –î–Ω—ñ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—â–∏–Ω—ñ —Ç–∞ –ó–∞–ø–æ—Ä—ñ–∂–∂—ñ —á–∞—Å—Ç–∫–æ–≤–æ –≤—ñ–¥–Ω–æ–≤–ª—é—é—Ç—å —Å–≤—ñ—Ç–ª–æ: —è–∫–∞ –∑–∞—Ä–∞–∑ —Å–∏—Ç—É–∞—Ü—ñ—è –≤ —Ä–µ–≥—ñ–æ–Ω–∞—Ö",
        "‚Äú–ù–∞–¥–∑–≤–∏—á–∞–π–Ω–∞ —Å–∏—Ç—É–∞—Ü—ñ—è –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è‚Äù: –º–µ—Ä –î–Ω—ñ–ø—Ä–∞ –§—ñ–ª–∞—Ç–æ–≤ —Ä–æ–∑–ø–æ–≤—ñ–≤ –ø—Ä–æ —Å—Ç–∞–Ω —Å–ø—Ä–∞–≤ –≤ –º—ñ—Å—Ç—ñ –ø—ñ—Å–ª—è –±–ª–µ–∫–∞—É—Ç—É",
        "–†–æ—Å—ñ–π—Å—å–∫—ñ –∞—Ç–∞–∫–∏ –∑–∞–ª–∏—à–∏–ª–∏ –±–µ–∑ —Å–≤—ñ—Ç–ª–∞ –ó–∞–ø–æ—Ä—ñ–∑—å–∫—É —Ç–∞ –î–Ω—ñ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å—å–∫—É –æ–±–ª–∞—Å—Ç—ñ ",
        "–ó–∞–ø–æ—Ä—ñ–∂–∂—è –∑–Ω–æ–≤—É –∑—ñ —Å–≤—ñ—Ç–ª–æ–º –ø—ñ—Å–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –±–ª–µ–∫–∞—É—Ç—É. –£ –î–Ω—ñ–ø—Ä—ñ –µ–ª–µ–∫—Ç—Ä–æ–µ–Ω–µ—Ä–≥—ñ—ó –ø–æ–∫–∏ –Ω–µ–º–∞—î, —Å–∏—Ç—É–∞—Ü—ñ—è —Å–∫–ª–∞–¥–Ω–∞",
        "–ù–∞ –î–Ω—ñ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—â–∏–Ω—ñ –±–µ–∑ —Å—Ç—Ä—É–º—É –∑–∞–ª–∏—à–∏–ª–∏—Å—å 8 –≤—É–≥—ñ–ª—å–Ω–∏—Ö —à–∞—Ö—Ç (–í—ñ–¥–µ–æ)",
        "–¢—Ä–∞–º–ø –ø—ñ–¥—Ç—Ä–∏–º–∞–≤ –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç –ø—Ä–æ —Å–∞–Ω–∫—Ü—ñ—ó –ø—Ä–æ—Ç–∏ –†–æ—Å—ñ—ó",
        "–°–µ–Ω–∞—Ç–æ—Ä –ì—Ä—ç–º: –¢—Ä–∞–º–ø –æ–¥–æ–±—Ä–∏–ª –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç –æ –Ω–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏—è—Ö –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫—É—é –Ω–µ—Ñ—Ç—å",
        "–í –£–∫—Ä–∞—ó–Ω—ñ –≤—ñ–¥—Ä–µ–∞–≥—É–≤–∞–ª–∏ –Ω–∞ \"–∑–µ–ª–µ–Ω–µ —Å–≤—ñ—Ç–ª–æ\" –≤—ñ–¥ –¢—Ä–∞–º–ø–∞ –Ω–∞ —Å–∞–Ω–∫—Ü—ñ—ó –°–®–ê –ø—Ä–æ—Ç–∏ –†–æ—Å—ñ—ó",
        "–¢—Ä–∞–º–ø –¥–∞–≤ ¬´–∑–µ–ª–µ–Ω–µ —Å–≤—ñ—Ç–ª–æ¬ª —Å–∞–Ω–∫—Ü—ñ–π–Ω–æ–º—É –∑–∞–∫–æ–Ω–æ–ø—Ä–æ—î–∫—Ç—É –ø—Ä–æ—Ç–∏ —Ä–æ—Å—ñ—ó ‚Äî “ê—Ä–µ–º",
        "–¢—Ä–∞–º–ø –¥–æ–∑–≤–æ–ª–∏–≤ –ö–æ–Ω–≥—Ä–µ—Å—É –ø—Ä–æ—Å—É–Ω—É—Ç–∏ —Å–∞–Ω–∫—Ü—ñ–π–Ω–∏–π –∑–∞–∫–æ–Ω–æ–ø—Ä–æ—î–∫—Ç –ø—Ä–æ—Ç–∏ –ø–∞—Ä—Ç–Ω–µ—Ä—ñ–≤ –†–æ—Å—ñ—ó",
        "–®—É—Ñ—Ä–∏—á—É –¥–æ–∑–≤–æ–ª–∏–ª–∏ –≤–∏–π—Ç–∏ –∑ –°–Ü–ó–û –ø—ñ–¥ –∑–∞—Å—Ç–∞–≤—É",
        "–í–µ–ª–∏–∫–∞ –ë—Ä–∏—Ç–∞–Ω—ñ—è –ø–µ—Ä–µ–¥–∞–ª–∞ –£–∫—Ä–∞—ó–Ω—ñ 13 —Å–∏—Å—Ç–µ–º –ü–ü–û Raven —ñ —Ä–æ–∑–ø–æ—á–∞–ª–∞ –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è Gravehawk",
        "–ú–µ–Ω—à–µ, –Ω—ñ–∂ –æ—á—ñ–∫—É–≤–∞–ª–∏: —Å–∫—ñ–ª—å–∫–∏ —Å–≤–æ—ó—Ö —Å–æ–ª–¥–∞—Ç—ñ–≤ –∫—Ä–∞—ó–Ω–∏ –ó–∞—Ö–æ–¥—É –≥–æ—Ç–æ–≤—ñ –Ω–∞–ø—Ä–∞–≤–∏—Ç–∏ –≤ –£–∫—Ä–∞—ó–Ω—É",
        "–®—É—Ñ—Ä–∏—á—É –¥–æ–∑–≤–æ–ª–∏–ª–∏ –≤–∏–π—Ç–∏ –∑-–ø—ñ–¥ –≤–∞—Ä—Ç–∏ –ø—ñ–¥ –∑–∞—Å—Ç–∞–≤—É —É –ø–æ–Ω–∞–¥ 33 –º–ª–Ω –≥—Ä–Ω",
        "–°—É–¥ –≤–∏–ø—É—Å—Ç–∏–≤ –®—É—Ñ—Ä–∏—á–∞ –∑-–ø—ñ–¥ –≤–∞—Ä—Ç–∏",
    ]

    texts = [text[:1000] for text in _texts]

    model = SentenceTransformer(
        # "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        # "sentence-transformers/all-MiniLM-L6-v2"
        "google/embeddinggemma-300m"
        # "Qwen/Qwen3-Embedding-0.6B"
        # "Qwen/Qwen3-Embedding-8B"
    )

    print("‚ÑπÔ∏è Calculating embeddings...")
    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True  # –í–ê–ñ–õ–ò–í–û –¥–ª—è HDBSCAN
    )
    
    # for e in embeddings:
    #     pprint(e)

    print("‚ÑπÔ∏è Clustering...")
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=3,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        min_samples=2,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É

        # min_cluster_size=7,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        # min_samples=3,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É

        # min_cluster_size=5,      # –º—ñ–Ω. —Ä–æ–∑–º—ñ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        # min_samples=3,           # —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É
        metric="euclidean",      # –∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ = cosine
        cluster_selection_method="eom"
    )

    labels = clusterer.fit_predict(embeddings)

    # –≥—Ä—É–ø—É—î–º–æ —Ç–µ–∫—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö üì¶
    clusters = {}
    for text, label in zip(texts, labels):
        clusters.setdefault(label, []).append(text)

    # —Å–æ—Ä—Ç—É—î–º–æ –∫–ª–∞—Å—Ç–µ—Ä–∏ –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—é —Ç–µ–∫—Å—Ç—ñ–≤ (—Å–ø–∞–¥–∞–Ω–Ω—è ‚¨áÔ∏è)
    sorted_clusters = sorted(
        clusters.items(),
        key=lambda item: len(item[1]),
        reverse=True
    )

    # –≤–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç üñ®Ô∏è
    for index, (label, items) in enumerate(sorted_clusters, 1):
        if label == -1:
                continue
        print(f"\nCLUSTER {index} (label {label}) ({len(items)})")
        pprint([item[:200] for item in items[:10]])
        # print(f"\nCLUSTER {label} ({len(items)})")
        # print(items[0][:300])

    # clusters = {}
    # for text, label in zip(texts, labels):
    #     clusters.setdefault(label, []).append(text)

    # for label, items in clusters.items():
    #     if label == -1:
    #         continue
    #     print(f"\nCLUSTER {label} ({len(items)})")
    #     pprint([item[:200] for item in items])
    #     # print(items[0][:300])

    pprint(Counter(labels))


if __name__ == "__main__":
    main()
